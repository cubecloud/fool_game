{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fool_game_and_TD3_env_steps_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AUNYbmtaU8Fm",
        "XVB66D2S1epw"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cubecloud/fool_game/blob/master/fool_game_and_TD3_env_steps_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkKWFvKdjz81"
      },
      "source": [
        "### New Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrsJc8Gx6nvo",
        "outputId": "742f77ba-4a3a-4eea-8dd6-06d260af9b0d"
      },
      "source": [
        "!pip install git+https://github.com/cubecloud/fool_game.git@feature-07-td3-refactoring"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/cubecloud/fool_game.git@feature-07-td3-refactoring\n",
            "  Cloning https://github.com/cubecloud/fool_game.git (to revision feature-07-td3-refactoring) to /tmp/pip-req-build-qrzb9n96\n",
            "  Running command git clone -q https://github.com/cubecloud/fool_game.git /tmp/pip-req-build-qrzb9n96\n",
            "  Running command git checkout -b feature-07-td3-refactoring --track origin/feature-07-td3-refactoring\n",
            "  Switched to a new branch 'feature-07-td3-refactoring'\n",
            "  Branch 'feature-07-td3-refactoring' set up to track remote branch 'feature-07-td3-refactoring' from 'origin'.\n",
            "Requirement already satisfied (use --upgrade to upgrade): fool-game==0.2.65 from git+https://github.com/cubecloud/fool_game.git@feature-07-td3-refactoring in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: setuptools>=51.0.0 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.65) (56.1.0)\n",
            "Requirement already satisfied: tensorboardx>=2.2 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.65) (2.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.65) (2018.9)\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.65) (3.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.65) (0.11.1)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.65) (0.6)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.65) (1.1.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx>=2.2->fool-game==0.2.65) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx>=2.2->fool-game==0.2.65) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->fool-game==0.2.65) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->fool-game==0.2.65) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->fool-game==0.2.65) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->fool-game==0.2.65) (2.4.7)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.11.1->fool-game==0.2.65) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx>=2.2->fool-game==0.2.65) (1.15.0)\n",
            "Building wheels for collected packages: fool-game\n",
            "  Building wheel for fool-game (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fool-game: filename=fool_game-0.2.65-cp37-none-any.whl size=29470 sha256=d34e3dc8431d20103a3615a8eb3edb63826b74467ebb8c608458f636c96b64fb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f62e4xjt/wheels/38/c2/4e/28e56445c7bf68d7fa9b8fd53058209ed98fe03bf84a54a441\n",
            "Successfully built fool-game\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acpTAjrvSGZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0324353b-3819-4c5a-8157-0cf61883c733"
      },
      "source": [
        "# adding terra_ai location for using googlesync \n",
        "# for development on local drive and testing \n",
        "# in google colab or jupyter notebook \n",
        "# change this variables to your locations for development\n",
        "local_drive = '/home/cubecloud/GDrive'\n",
        "remote_drive = '/content/drive/MyDrive'\n",
        "local_dev = '/Python/fool_game/'\n",
        "remote_dev = '/Python/fool_game/'\n",
        "__demo_version__ = \"0.1.0\"\n",
        "import sys\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    DEV = f'{remote_drive}{remote_dev}'\n",
        "    DRIVE = remote_drive\n",
        "elif 'ipykernel' in str(get_ipython()):\n",
        "    print('Running on Jupyter Notebook')\n",
        "    DEV = f'{local_drive}{local_dev}'\n",
        "    DRIVE = local_drive\n",
        "else:\n",
        "    sys.exit('Not running on CoLab or Jupyter notebook')\n",
        "print(f'Adding sys path: {DEV}')\n",
        "sys.path.append(DEV)\n",
        "HOME = f'{DEV}data/'\n",
        "\n",
        "#check environment \n",
        "\n",
        "import tensorflow\n",
        "print('Checking key environment depenndecies')\n",
        "!python --version\n",
        "print('TensorFlow', tensorflow.__version__)\n",
        "print('Keras', tensorflow.keras.__version__)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n",
            "Adding sys path: /content/drive/MyDrive/Python/fool_game/\n",
            "Checking key environment depenndecies\n",
            "Python 3.7.10\n",
            "TensorFlow 2.5.0\n",
            "Keras 2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J1alVgtUgUf",
        "outputId": "7bf9f810-39aa-4eb8-ab82-0444658bc9a6"
      },
      "source": [
        "import collections\n",
        "import numpy as np # импортируем библиотеку для работы с массивами данных\n",
        "import tensorflow as tf\n",
        "# from tensorflow.keras.models import Model, load_model \n",
        "# from tensorflow.keras import layers\n",
        "# from tensorflow.keras.layers import Dense, Flatten, Input, Lambda, Conv2D, MaxPooling2D, Reshape, Multiply # из кераса загружаем необходимые слои для нейросети\n",
        "# from tensorflow.keras.layers import BatchNormalization\n",
        "# from tensorflow.keras.optimizers import RMSprop, Adam, SGD, RMSprop# из кераса загружаем выбранный оптимизатор\n",
        "import time                                # модуль для операций со временными характеристиками\n",
        "import matplotlib.pyplot as plt            # импортируем библиотеку для визуализации данных\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import clear_output \n",
        "\n",
        "import pytz\n",
        "timezone = pytz.timezone(\"Europe/Moscow\")\n",
        "# # \"магическая\" команда python для запуска библиотеки в ноутбуке\n",
        "# %matplotlib inline\n",
        "\n",
        "HOME = f'/content/drive/MyDrive/Python/fool_game/data/'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from importlib import reload\n",
        "from cardgames import foolgame\n",
        "foolgame = reload(foolgame)\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)\n",
        "print(foolgame.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2.5.0\n",
            "2.5.0\n",
            "0.02.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUNYbmtaU8Fm"
      },
      "source": [
        "### Figshow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_qf7NMJwVjs"
      },
      "source": [
        "def figshow(df):\n",
        "    fig = plt.figure(figsize=(26, 7))\n",
        "    sns.set_style(\"white\")\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.set_axisbelow(True)\n",
        "    ax1.minorticks_on()\n",
        "    ax1.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "    ax1.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "    last_game = int(df[\"game_episode\"].max())\n",
        "    # N = np.arange(0, last_game+1)\n",
        "    n_games = 1\n",
        "    if last_game // 400 > 0:\n",
        "        n_games = last_game // 400\n",
        "    \n",
        "\n",
        "    plt.plot(df.loc[(df[\"game_episode\"] % n_games == 0), \"game_episode\"], \n",
        "             df.loc[(df[\"game_episode\"] % n_games == 0), \"loss\"], \n",
        "             linestyle='--', color='blue', label=\"loss\")\n",
        "    plt.plot(df.loc[(df[\"game_episode\"] % n_games == 0), \"game_episode\"], \n",
        "             df.loc[(df[\"game_episode\"] % n_games == 0), \"epsilon\"], \n",
        "             linestyle='--', color='green', label=\"epsilon\")\n",
        "    plt.title(f\"Loss & epsilon\")\n",
        "    plt.legend()\n",
        "\n",
        "    ax2 = fig.add_subplot(1, 2, 2)\n",
        "    ax2.set_axisbelow(True)\n",
        "    ax2.minorticks_on()\n",
        "    ax2.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "    ax2.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "    plt.plot(df.loc[(df[\"game_episode\"] % n_games == 0), \"game_episode\"],\n",
        "             df.loc[(df[\"game_episode\"] % n_games == 0), 'mean_reward'], \n",
        "             linestyle='-', color='red', label=\"mean_reward\")\n",
        "    plt.title(f\"mean_reward\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    pass"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7uAU9CHLGaa",
        "outputId": "61327ba5-6da0-4c0e-8e64-cca949bd8ec8"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed May 26 07:13:36 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wznv9I1KR_I3"
      },
      "source": [
        "## The DQN model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6B8v-Qh5Ykk"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn        # Pytorch neural network package\n",
        "import torch.optim as optim  # Pytorch optimization package\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "# device = torch.device(\"cuda\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqcYBObzuAvg"
      },
      "source": [
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"Initialize parameters and build model.\n",
        "        Args:\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            max_action (float): highest action to take\n",
        "            seed (int): Random seed\n",
        "            h1_units (int): Number of nodes in first hidden layer\n",
        "            h2_units (int): Number of nodes in second hidden layer\n",
        "            \n",
        "        Return:\n",
        "            action output of network with tanh activation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.max_action * torch.tanh(self.l3(x)) \n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twM4nrgJtrJB"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    \"\"\"Initialize parameters and build model.\n",
        "        Args:\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            max_action (float): highest action to take\n",
        "            seed (int): Random seed\n",
        "            h1_units (int): Number of nodes in first hidden layer\n",
        "            h2_units (int): Number of nodes in second hidden layer\n",
        "            \n",
        "        Return:\n",
        "            value output of network \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Q1 architecture\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "\n",
        "        # Q2 architecture\n",
        "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l5 = nn.Linear(400, 300)\n",
        "        self.l6 = nn.Linear(300, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "\n",
        "        x1 = F.relu(self.l1(xu))\n",
        "        x1 = F.relu(self.l2(x1))\n",
        "        x1 = self.l3(x1)\n",
        "\n",
        "        x2 = F.relu(self.l4(xu))\n",
        "        x2 = F.relu(self.l5(x2))\n",
        "        x2 = self.l6(x2)\n",
        "        return x1, x2\n",
        "\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "\n",
        "        x1 = F.relu(self.l1(xu))\n",
        "        x1 = F.relu(self.l2(x1))\n",
        "        x1 = self.l3(x1)\n",
        "        return x1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG-NB0oDiFEg"
      },
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0nn9NHw1Qa5"
      },
      "source": [
        "\n",
        "\n",
        "# Code based on: \n",
        "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
        "\n",
        "# Expects tuples of (state, next_state, action, reward, done)\n",
        "class ReplayBuffer(object):\n",
        "    \"\"\"Buffer to store tuples of experience replay\"\"\"\n",
        "    \n",
        "    def __init__(self, max_size=1000000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_size (int): total amount of tuples to store\n",
        "        \"\"\"\n",
        "        \n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "\n",
        "    def add(self, data):\n",
        "        \"\"\"Add experience tuples to buffer\n",
        "        \n",
        "        Args:\n",
        "            data (tuple): experience replay tuple\n",
        "        \"\"\"\n",
        "        \n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = data\n",
        "            self.ptr = (self.ptr + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(data)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samples a random amount of experiences from buffer of batch size\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int): size of sample\n",
        "        \"\"\"\n",
        "        \n",
        "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        states, actions, next_states, rewards, dones = [], [], [], [], []\n",
        "\n",
        "        for i in ind: \n",
        "            s, a, s_, r, d = self.storage[i]\n",
        "            states.append(np.array(s, copy=False))\n",
        "            actions.append(np.array(a, copy=False))\n",
        "            next_states.append(np.array(s_, copy=False))\n",
        "            rewards.append(np.array(r, copy=False))\n",
        "            dones.append(np.array(d, copy=False))\n",
        "\n",
        "        return np.array(states), np.array(actions), np.array(next_states), np.array(rewards).reshape(-1, 1), np.array(dones).reshape(-1, 1)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKOLyrAVhMwX"
      },
      "source": [
        "#### old experience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obhi7_P2RSpQ"
      },
      "source": [
        "# Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'valid_action_lst','reward', 'done', 'next_state'])\n",
        "\n",
        "# class ExperienceReplay:\n",
        "#     def __init__(self, capacity):\n",
        "#         self.capacity = capacity\n",
        "#         self.buffer = collections.deque(maxlen=capacity)\n",
        "#         pass\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.buffer)\n",
        "\n",
        "#     def get_length(self):\n",
        "#         return self.__len__()\n",
        "\n",
        "#     def append(self, experience):\n",
        "#         self.buffer.append(experience)\n",
        "#         pass\n",
        "\n",
        "#     def sample(self, batch_size):\n",
        "#         indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "#         states, actions, valid_actions_lst, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "#         return (np.array(states, dtype=np.float32), \n",
        "#                 np.array(actions, dtype=np.int64), \n",
        "#                 np.array(valid_actions_lst), \n",
        "#                 np.array(rewards, dtype=np.float32).reshape(-1, 1),\n",
        "#                 np.array(dones, dtype=np.uint8).reshape(-1, 1), \n",
        "#                 np.array(next_states, dtype=np.float32))\n",
        "\n",
        "#     def save(self, file_path, buffer_length=10000):\n",
        "#         len_buffer = len(self.buffer)\n",
        "#         with open(file_path, \"wb\") as f:\n",
        "#             print('Save exp buffer...')\n",
        "#             if not (self.capacity is None) \\\n",
        "#                     and (len_buffer < self.capacity) \\\n",
        "#                     and (len_buffer < buffer_length):\n",
        "#                 buffer_length = len_buffer\n",
        "#             else:\n",
        "#                 buffer_length = len_buffer\n",
        "#             states, actions, valid_actions_lst, rewards, dones, next_states = \\\n",
        "#                 zip(*[self.buffer[idx] for idx in range(len(self.buffer) - buffer_length, len(self.buffer))])\n",
        "#             pkl.dump([states, actions, valid_actions_lst, rewards, dones, next_states], f)\n",
        "#             del [states, actions, valid_actions_lst, rewards, dones, next_states]\n",
        "#             pass\n",
        "\n",
        "#     def load(self, file_path):\n",
        "#         with open(file_path, \"rb\") as f:\n",
        "#             print('Loading exp buffer...')\n",
        "#             # self.buffer = pkl.load(f)\n",
        "#             states, actions, valid_actions_lst, rewards, dones, next_states = pkl.load(f)\n",
        "#             for state, action, valid_action_lst, reward, done, next_state in zip(states, actions, valid_actions_lst, rewards, dones, next_states):\n",
        "#                 exp = Experience(state, action, valid_action_lst, reward, done, next_state)\n",
        "#                 self.buffer.append(exp)\n",
        "#             del [states, actions, valid_actions_lst, rewards, dones, next_states]\n",
        "#         pass"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Z2t-QEhSva"
      },
      "source": [
        "### TD3 Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCx88PAJzTmH"
      },
      "source": [
        "class TD3(object):\n",
        "    \"\"\"Agent class that handles the training of the networks and provides outputs as actions\n",
        "    \n",
        "        Args:\n",
        "            state_dim (int): state size\n",
        "            action_dim (int): action size\n",
        "            max_action (float): highest action to take\n",
        "            device (device): cuda or cpu to process tensors\n",
        "            env (env): gym environment to use\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, max_action, env):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.env = env\n",
        "\n",
        "        \n",
        "    def select_action(self, state, noise=0.1):\n",
        "        \"\"\"Select an appropriate action from the agent policy\n",
        "        \n",
        "            Args:\n",
        "                state (array): current state of environment\n",
        "                noise (float): how much noise to add to acitons\n",
        "                \n",
        "            Returns:\n",
        "                action (float): action clipped within action range\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        \n",
        "        action = self.actor(state).cpu().data.numpy().flatten()\n",
        "        # print('action from actor',action)\n",
        "        if noise != 0: \n",
        "            action = (action + np.random.normal(0, noise, size=action.shape))\n",
        "            # action = (action + np.random.normal(0, noise, size=self.env.action_space.shape[0]))\n",
        "        # print('action before clip', action)\n",
        "        # action = action.clip(self.env.action_space.low, self.env.action_space.high)\n",
        "        action = action.clip(-1.0, 1.0)\n",
        "        # print('action after clip', action)\n",
        "        return action\n",
        "\n",
        "    \n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "        \"\"\"Train and update actor and critic networks\n",
        "        \n",
        "            Args:\n",
        "                replay_buffer (ReplayBuffer): buffer for experience replay\n",
        "                iterations (int): how many times to run training\n",
        "                batch_size(int): batch size to sample from replay buffer\n",
        "                discount (float): discount factor\n",
        "                tau (float): soft update for main networks to target networks\n",
        "                \n",
        "            Return:\n",
        "                actor_loss (float): loss from actor network\n",
        "                critic_loss (float): loss from critic network\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        for it in range(iterations):\n",
        "\n",
        "            # Sample replay buffer \n",
        "            x, y, u, r, d = replay_buffer.sample(batch_size)\n",
        "\n",
        "            state = torch.FloatTensor(x).to(device)\n",
        "            action = torch.FloatTensor(u).to(device)\n",
        "            next_state = torch.FloatTensor(y).to(device)\n",
        "            done = torch.FloatTensor(1 - d).to(device)\n",
        "            reward = torch.FloatTensor(r).to(device)\n",
        "\n",
        "            # Select action according to policy and add clipped noise \n",
        "            noise = torch.FloatTensor(u).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Compute the target Q value\n",
        "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + (done * discount * target_Q).detach()\n",
        "\n",
        "            # Get current Q estimates\n",
        "            current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "            # Compute critic loss\n",
        "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q) \n",
        "\n",
        "            # Optimize the critic\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            # Delayed policy updates\n",
        "            if it % policy_freq == 0:\n",
        "\n",
        "                # Compute actor loss\n",
        "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "                # Optimize the actor \n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # Update the frozen target models\n",
        "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "\n",
        "    def load(self, filename=\"best_avg\", directory=\"./saves\"):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfdza2EyzxCg"
      },
      "source": [
        "class Runner():\n",
        "    \"\"\"Carries out the environment steps and adds experiences to memory\"\"\"\n",
        "    \n",
        "    def __init__(self, env, agent, replay_buffer):\n",
        "        \n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.obs = self.env.reset()\n",
        "        self.done = False\n",
        "        self._reset()\n",
        "        pass\n",
        "\n",
        "    def _reset(self):\n",
        "        self.obs = self.env.reset()\n",
        "        # self.env.step(0, first_step=True)\n",
        "        pass\n",
        "\n",
        "    def next_step(self, episode_timesteps, noise=0.1):\n",
        "        action = self.agent.select_action(np.array(self.obs), noise=0.1)\n",
        "\n",
        "        if not self.env.action_space.is_this_valid_action_ohe(action):\n",
        "            reward = -0.007\n",
        "            done_bool = False\n",
        "            replay_buffer.add((self.obs, self.obs, action, reward, done_bool))\n",
        "            action = self.env.action_space.sample_ohe()\n",
        "            # print(action)\n",
        "        \n",
        "        # Perform action\n",
        "        new_obs, reward, done, info = self.env.step(action) \n",
        "        done_bool = 0 if episode_timesteps + 1 == 200 else float(done)\n",
        "        # print(info)\n",
        "        # Store data in replay buffer\n",
        "        replay_buffer.add((self.obs, new_obs, action, reward, done_bool))\n",
        "        \n",
        "        self.obs = new_obs\n",
        "        \n",
        "        if done:\n",
        "            # self.obs = self.env.reset()\n",
        "            done = False\n",
        "            # print('Done')\n",
        "            self._reset()\n",
        "            return reward, True\n",
        "        \n",
        "        return reward, done"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vF_vMKRz4NV"
      },
      "source": [
        "def evaluate_policy(policy, env, eval_episodes=100,render=False):\n",
        "    \"\"\"run several episodes using the best agent policy\n",
        "        \n",
        "        Args:\n",
        "            policy (agent): agent to evaluate\n",
        "            env (env): gym environment\n",
        "            eval_episodes (int): how many test episodes to run\n",
        "            render (bool): show training\n",
        "        \n",
        "        Returns:\n",
        "            avg_reward (float): average reward over the number of evaluations\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    avg_reward = 0.\n",
        "    for i in range(eval_episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if render:\n",
        "                env.render()\n",
        "            action = policy.select_action(np.array(obs), noise=0)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            avg_reward += reward\n",
        "\n",
        "    avg_reward /= eval_episodes\n",
        "\n",
        "    print(\"\\n---------------------------------------\")\n",
        "    print(\"Evaluation over {:d} episodes: {:f}\" .format(eval_episodes, avg_reward))\n",
        "    print(\"---------------------------------------\")\n",
        "    return avg_reward"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-F_ds0V0J7-"
      },
      "source": [
        "def observe(env,replay_buffer, observation_steps):\n",
        "    \"\"\"run episodes while taking random actions and filling replay_buffer\n",
        "    \n",
        "        Args:\n",
        "            env (env): gym environment\n",
        "            replay_buffer(ReplayBuffer): buffer to store experience replay\n",
        "            observation_steps (int): how many steps to observe for\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    time_steps = 0\n",
        "    obs = env.reset()\n",
        "    # env.step(0, first_step=True)\n",
        "    # obs = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while time_steps < observation_steps:\n",
        "        action = env.action_space.sample_ohe()\n",
        "        if not env.action_space.is_this_valid_action_ohe(action):\n",
        "            reward = -0.007\n",
        "            done_bool = False\n",
        "            replay_buffer.add((obs, obs, action, reward, done_bool))\n",
        "            action = env.action_space.sample_ohe()\n",
        "        # print('Valid actions\\n', env.action_space.pl.analyze())\n",
        "        # print('Action from sample\\n', action)\n",
        "        # print('Desktop\\n', env.action_space.pl.desktop_list)\n",
        "        # print('Cards on hand\\n', env.action_space.pl.player_cards_onhand_list)\n",
        "        new_obs, reward, done, info = env.step(action)\n",
        "        # print(info)\n",
        "        replay_buffer.add((obs, new_obs, action, reward, done))\n",
        "        # assert new_obs.shape == obs.shape, f'Error in states dimensions {new_obs.shape} != {obs.shape}'\n",
        "        obs = new_obs\n",
        "        time_steps += 1\n",
        "\n",
        "        if done:\n",
        "            obs = env.reset()\n",
        "            # env.step(0, first_step=True)\n",
        "            done = False\n",
        "\n",
        "        print(\"\\rPopulating Buffer {}/{}.\".format(time_steps, observation_steps), end=\"\")\n",
        "        sys.stdout.flush()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TD7RrsF0ORL"
      },
      "source": [
        "def train(agent, test_env):\n",
        "    \"\"\"Train the agent for exploration steps\n",
        "    \n",
        "        Args:\n",
        "            agent (Agent): agent to use\n",
        "            env (environment): gym environment\n",
        "            writer (SummaryWriter): tensorboard writer\n",
        "            exploration (int): how many training steps to run\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    total_timesteps = 0\n",
        "    timesteps_since_eval = 0\n",
        "    episode_num = 0\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    done = False \n",
        "    obs = env.reset()\n",
        "    evaluations = []\n",
        "    rewards = []\n",
        "    best_avg = -2000\n",
        "    \n",
        "    writer = SummaryWriter(comment=\"-TD3_foolgame_state_v006\")\n",
        "    \n",
        "    while total_timesteps < EXPLORATION:\n",
        "    \n",
        "        if done: \n",
        "\n",
        "            if total_timesteps != 0: \n",
        "                rewards.append(episode_reward)\n",
        "                avg_reward = np.mean(rewards[-100:])\n",
        "                \n",
        "                writer.add_scalar(\"avg_reward\", avg_reward, total_timesteps)\n",
        "                writer.add_scalar(\"reward_step\", reward, total_timesteps)\n",
        "                writer.add_scalar(\"episode_reward\", episode_reward, total_timesteps)\n",
        "                \n",
        "                if best_avg < avg_reward and len(rewards)>100:\n",
        "                    best_avg = avg_reward\n",
        "                    print(f\"saving best model... Best average = {best_avg:.4f} at {datetime.now(timezone)}\\n\")\n",
        "                    agent.save(\"best_avg\", HOME)\n",
        "                    # agent.save(\"best_avg\",\"saves\")\n",
        "\n",
        "                print(\"\\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f} \".format(\n",
        "                    total_timesteps, episode_num, episode_reward, avg_reward), end=\"\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "\n",
        "                if avg_reward >= REWARD_THRESH:\n",
        "                    break\n",
        "\n",
        "                agent.train(replay_buffer, episode_timesteps, BATCH_SIZE, GAMMA, TAU, NOISE, NOISE_CLIP, POLICY_FREQUENCY)\n",
        "\n",
        "                # Evaluate episode\n",
        "#                 if timesteps_since_eval >= EVAL_FREQUENCY:\n",
        "#                     timesteps_since_eval %= EVAL_FREQUENCY\n",
        "#                     eval_reward = evaluate_policy(agent, test_env)\n",
        "#                     evaluations.append(avg_reward)\n",
        "#                     writer.add_scalar(\"eval_reward\", eval_reward, total_timesteps)\n",
        "\n",
        "#                     if best_avg < eval_reward:\n",
        "#                         best_avg = eval_reward\n",
        "#                         print(\"saving best model....\\n\")\n",
        "#                         agent.save(\"best_avg\",\"saves\")\n",
        "\n",
        "                episode_reward = 0\n",
        "                episode_timesteps = 0\n",
        "                episode_num += 1 \n",
        "\n",
        "        reward, done = runner.next_step(episode_timesteps)\n",
        "        episode_reward += reward\n",
        "\n",
        "        episode_timesteps += 1\n",
        "        total_timesteps += 1\n",
        "        timesteps_since_eval += 1\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgbjZZHOa9Y7"
      },
      "source": [
        "# ENV = \"RoboschoolHalfCheetah-v1\"#\"Pendulum-v0\"\n",
        "SEED = 0\n",
        "OBSERVATION = 15000\n",
        "EXPLORATION = 5000000\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "TAU = 0.0005\n",
        "NOISE = 0.2\n",
        "NOISE_CLIP = 0.5\n",
        "EXPLORE_NOISE = 0.1\n",
        "POLICY_FREQUENCY = 2\n",
        "EVAL_FREQUENCY = 5000\n",
        "REWARD_THRESH = 8000"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQOZPszNbDz5"
      },
      "source": [
        "# env = gym.make(ENV)\n",
        "players_num = 2\n",
        "env = foolgame.Environment(players_num,\n",
        "                           env_type=\"Dummy\",\n",
        "                           observer_player=1,\n",
        "                           nnmodel=None)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set seeds\n",
        "# env.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# state_dim = env.observation_space.shape[0]\n",
        "state_dim = env.observation_space.states_dim\n",
        "action_dim = env.action_space.shape[0] \n",
        "# max_action = float(env.action_space.high[0])\n",
        "''' max action probability from neurons '''\n",
        "max_action = env.action_space.abs_minmax\n",
        "\n",
        "\n",
        "policy = TD3(state_dim, action_dim, max_action, env)\n",
        "\n",
        "replay_buffer = ReplayBuffer()\n",
        "\n",
        "runner = Runner(env, policy, replay_buffer)\n",
        "\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdYrZwZafNrf",
        "outputId": "b229df07-44da-4f6e-e653-17d446bbf4e9"
      },
      "source": [
        "# Populate replay buffer\n",
        "observe(env, replay_buffer, OBSERVATION)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating Buffer 15000/15000."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdjFvFDPfSbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40d7460-a359-461a-8b63-c1d9ce7b7c92"
      },
      "source": [
        "train(policy, env)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total T: 4365 Episode Num: 99 Reward: -0.878000 Avg Reward: 0.083330 saving best model... Best average = 0.0636 at 2021-05-26 10:14:44.202922+03:00\n",
            "\n",
            "Total T: 4411 Episode Num: 100 Reward: -0.875000 Avg Reward: 0.063560 saving best model... Best average = 0.0636 at 2021-05-26 10:14:44.508794+03:00\n",
            "\n",
            "Total T: 4546 Episode Num: 103 Reward: -0.861000 Avg Reward: 0.043280 saving best model... Best average = 0.0639 at 2021-05-26 10:14:45.309784+03:00\n",
            "\n",
            "Total T: 5033 Episode Num: 113 Reward: 1.124000 Avg Reward: 0.045110 saving best model... Best average = 0.0662 at 2021-05-26 10:14:47.982529+03:00\n",
            "\n",
            "Total T: 5083 Episode Num: 114 Reward: 1.173000 Avg Reward: 0.066220 saving best model... Best average = 0.0867 at 2021-05-26 10:14:48.336056+03:00\n",
            "\n",
            "Total T: 5170 Episode Num: 116 Reward: -0.884000 Avg Reward: 0.066710 saving best model... Best average = 0.0870 at 2021-05-26 10:14:48.876616+03:00\n",
            "\n",
            "Total T: 5217 Episode Num: 117 Reward: 1.160000 Avg Reward: 0.086950 saving best model... Best average = 0.1073 at 2021-05-26 10:14:49.215599+03:00\n",
            "\n",
            "Total T: 5402 Episode Num: 121 Reward: -0.878000 Avg Reward: 0.087410 saving best model... Best average = 0.1080 at 2021-05-26 10:14:50.283342+03:00\n",
            "\n",
            "Total T: 5459 Episode Num: 122 Reward: 1.178000 Avg Reward: 0.108040 saving best model... Best average = 0.1280 at 2021-05-26 10:14:50.643798+03:00\n",
            "\n",
            "Total T: 5500 Episode Num: 123 Reward: 1.114000 Avg Reward: 0.127960 saving best model... Best average = 0.1280 at 2021-05-26 10:14:50.939850+03:00\n",
            "\n",
            "Total T: 5539 Episode Num: 124 Reward: -0.882000 Avg Reward: 0.128020 saving best model... Best average = 0.1485 at 2021-05-26 10:14:51.222780+03:00\n",
            "\n",
            "Total T: 5573 Episode Num: 125 Reward: 1.127000 Avg Reward: 0.148500 saving best model... Best average = 0.1688 at 2021-05-26 10:14:51.528581+03:00\n",
            "\n",
            "Total T: 5620 Episode Num: 126 Reward: 1.152000 Avg Reward: 0.168810 saving best model... Best average = 0.1895 at 2021-05-26 10:14:51.874205+03:00\n",
            "\n",
            "Total T: 5666 Episode Num: 127 Reward: 1.133000 Avg Reward: 0.189520 saving best model... Best average = 0.1904 at 2021-05-26 10:14:52.231105+03:00\n",
            "\n",
            "Total T: 5793 Episode Num: 129 Reward: -0.860000 Avg Reward: 0.170530 saving best model... Best average = 0.1909 at 2021-05-26 10:14:52.990948+03:00\n",
            "\n",
            "Total T: 10922 Episode Num: 244 Reward: 1.132000 Avg Reward: 0.185850 saving best model... Best average = 0.2055 at 2021-05-26 10:15:20.575712+03:00\n",
            "\n",
            "Total T: 11281 Episode Num: 252 Reward: -0.882000 Avg Reward: 0.185860 saving best model... Best average = 0.2068 at 2021-05-26 10:15:22.594603+03:00\n",
            "\n",
            "Total T: 11332 Episode Num: 253 Reward: 1.166000 Avg Reward: 0.206760 saving best model... Best average = 0.2068 at 2021-05-26 10:15:22.934048+03:00\n",
            "\n",
            "Total T: 11367 Episode Num: 254 Reward: 1.114000 Avg Reward: 0.206810 saving best model... Best average = 0.2265 at 2021-05-26 10:15:23.203893+03:00\n",
            "\n",
            "Total T: 11666 Episode Num: 261 Reward: -0.897000 Avg Reward: 0.206450 saving best model... Best average = 0.2270 at 2021-05-26 10:15:24.866639+03:00\n",
            "\n",
            "Total T: 11704 Episode Num: 262 Reward: 1.143000 Avg Reward: 0.227040 saving best model... Best average = 0.2271 at 2021-05-26 10:15:25.185893+03:00\n",
            "\n",
            "Total T: 12283 Episode Num: 274 Reward: 1.131000 Avg Reward: 0.207640 saving best model... Best average = 0.2274 at 2021-05-26 10:15:28.312413+03:00\n",
            "\n",
            "Total T: 16790 Episode Num: 377 Reward: -0.908000 Avg Reward: 0.226760 saving best model... Best average = 0.2278 at 2021-05-26 10:15:52.570273+03:00\n",
            "\n",
            "Total T: 16917 Episode Num: 379 Reward: 1.131000 Avg Reward: 0.227360 saving best model... Best average = 0.2470 at 2021-05-26 10:15:53.312997+03:00\n",
            "\n",
            "Total T: 17175 Episode Num: 385 Reward: -0.892000 Avg Reward: 0.226530 saving best model... Best average = 0.2472 at 2021-05-26 10:15:54.797832+03:00\n",
            "\n",
            "Total T: 17489 Episode Num: 392 Reward: 1.109000 Avg Reward: 0.246200 saving best model... Best average = 0.2665 at 2021-05-26 10:15:56.532025+03:00\n",
            "\n",
            "Total T: 17539 Episode Num: 393 Reward: 1.131000 Avg Reward: 0.266520 saving best model... Best average = 0.2666 at 2021-05-26 10:15:56.875546+03:00\n",
            "\n",
            "Total T: 17866 Episode Num: 400 Reward: 1.169000 Avg Reward: 0.247110 saving best model... Best average = 0.2678 at 2021-05-26 10:15:58.710914+03:00\n",
            "\n",
            "Total T: 17901 Episode Num: 401 Reward: 1.150000 Avg Reward: 0.267790 saving best model... Best average = 0.2680 at 2021-05-26 10:15:58.992543+03:00\n",
            "\n",
            "Total T: 17947 Episode Num: 402 Reward: 1.151000 Avg Reward: 0.268000 saving best model... Best average = 0.2681 at 2021-05-26 10:15:59.314518+03:00\n",
            "\n",
            "Total T: 17998 Episode Num: 403 Reward: -0.844000 Avg Reward: 0.268070 saving best model... Best average = 0.2681 at 2021-05-26 10:15:59.655568+03:00\n",
            "\n",
            "Total T: 18095 Episode Num: 405 Reward: 1.133000 Avg Reward: 0.267720 saving best model... Best average = 0.2880 at 2021-05-26 10:16:00.260193+03:00\n",
            "\n",
            "Total T: 18142 Episode Num: 406 Reward: 1.130000 Avg Reward: 0.287960 saving best model... Best average = 0.2884 at 2021-05-26 10:16:01.008008+03:00\n",
            "\n",
            "Total T: 18194 Episode Num: 407 Reward: 1.171000 Avg Reward: 0.288400 saving best model... Best average = 0.3085 at 2021-05-26 10:16:01.374075+03:00\n",
            "\n",
            "Total T: 18235 Episode Num: 408 Reward: 1.124000 Avg Reward: 0.308460 saving best model... Best average = 0.3287 at 2021-05-26 10:16:01.672354+03:00\n",
            "\n",
            "Total T: 18278 Episode Num: 409 Reward: 1.104000 Avg Reward: 0.328670 saving best model... Best average = 0.3487 at 2021-05-26 10:16:01.989332+03:00\n",
            "\n",
            "Total T: 18363 Episode Num: 411 Reward: -0.929000 Avg Reward: 0.347880 saving best model... Best average = 0.3680 at 2021-05-26 10:16:02.558749+03:00\n",
            "\n",
            "Total T: 283007 Episode Num: 6313 Reward: -0.852000 Avg Reward: 0.022220 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b4poO89RXCh"
      },
      "source": [
        "!tensorboard --logdir= runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVB66D2S1epw"
      },
      "source": [
        "# Pytorch *Agent*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swvZuUju1epz"
      },
      "source": [
        "players_num = 2\n",
        "num_actions=37\n",
        "\n",
        "# sync_target_turns = 3000       \n",
        "replay_start_size = 10000       # start replay at turn_idx\n",
        "\n",
        "model = DQN((518,), 37).to(device)\n",
        "target_model = DQN((518,), 37).to(device)\n",
        "\n",
        "# print(next(model1.parameters()).device)\n",
        "# print(next(model2.parameters()).device)\n",
        "# net = DQN((4536), 37).to(device)\n",
        "# target_net = DQN((4536), 37).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OjeqOkw1epx"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, \n",
        "                 env, \n",
        "                 exp_buffer,\n",
        "                 exp_buffer_size = 50000):\n",
        "        self.env = env\n",
        "        self.observer_player = env.observer_player\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.exp_buffer_size = exp_buffer_size\n",
        "        self.verbose = False\n",
        "        self.debug_verbose = 1\n",
        "        self.gamma = 0.99\n",
        "        self._reset()\n",
        "        pass\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = np.reshape(self.env.reset(), 518)\n",
        "        self.total_reward: float = 0.0\n",
        "        self.env.verbose = self.verbose\n",
        "        self.env.debug_verbose = self.debug_verbose\n",
        "        self.env.step(0, first_step=True)\n",
        "        self.turns_counter = 0\n",
        "        pass\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, step_device=\"cpu\"):\n",
        "        done_reward = None\n",
        "        self.turns_counter += 1\n",
        "        self.env.epsilon = epsilon\n",
        "        step_valid_actions = self.env.pl[self.observer_player].analyze()\n",
        "        if np.random.random() < epsilon:\n",
        "            action = random.choice(step_valid_actions)\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(step_device)\n",
        "            q_vals_v = net.forward(state_v)\n",
        "            # action = np.argmax(q_vals_v.cpu().detach().numpy())\n",
        "            # print(next(net.parameters()).device)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "            # state_a = np.array([self.state], copy=False)\n",
        "            # state = torch.FloatTensor(state_a).float().unsqueeze(0).to(device)\n",
        "            # qvals = net.forward(state)\n",
        "            # action = np.argmax(qvals.cpu().detach().numpy())\n",
        "\n",
        "            # state_a = np.array([self.state], copy=False)\n",
        "            # state_tensor = tf.convert_to_tensor(state_a)\n",
        "            # q_values = nnmodel(state_tensor, training=False)\n",
        "            # # with np.printoptions(precision=3, suppress=True):\n",
        "            # #     print(q_values.numpy())\n",
        "            # valid_masks = tf.one_hot(valid_action_list, self.env.num_actions)\n",
        "            # # with np.printoptions(precision=3, suppress=True):\n",
        "            # #     print(masks.numpy())\n",
        "            # valid_q_values = tf.expand_dims(tf.reduce_sum(tf.multiply(q_values, valid_masks), axis=0), 0)\n",
        "            # # with np.printoptions(precision=3, suppress=True):\n",
        "            # #     print(valid_q_values.numpy())\n",
        "            # action = np.argmax(valid_q_values)\n",
        "            # # print(self.action, action_list, action)\n",
        "        if not (action in step_valid_actions):\n",
        "            # action = step_valid_actions[0]\n",
        "            # action = random.choice(step_valid_actions)\n",
        "            ''' keeping the same state for new_state but adding negative reward'''\n",
        "            new_state = np.squeeze(state_a, axis=0)\n",
        "            self.state = np.squeeze(state_a, axis=0)\n",
        "            # print(new_state.shape)\n",
        "            # print(self.state.shape)\n",
        "            ''' negative reward for not valid action '''\n",
        "            reward = -0.007\n",
        "            is_done = False\n",
        "            self.total_reward += reward\n",
        "            exp = Experience(self.state, action, step_valid_actions, reward, is_done, new_state)\n",
        "            self.exp_buffer.append(exp)\n",
        "            action = random.choice(step_valid_actions)\n",
        "        ''' \n",
        "        step_epsilon - setting to 1.0 - totally random action from agent \n",
        "        trying to learn - how to play valid actions (index + zero (pass)) \n",
        "        '''\n",
        "        new_state, reward, is_done, info = self.env.step(action, step_epsilon=1.0)\n",
        "        new_state = np.reshape(new_state, 518)\n",
        "        # if self.verbose:\n",
        "            # print(\"Last action\", action)\n",
        "            # print(info)\n",
        "        self.total_reward += reward\n",
        "        exp = Experience(self.state, action, step_valid_actions, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or self.turns_counter > 100:\n",
        "            done_reward  = self.total_reward\n",
        "            self.verbose = False\n",
        "            self.debug_verbose = 1\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vghJS8ZV1ep0"
      },
      "source": [
        "MEAN_REWARD_BOUND = 0.85         \n",
        "\n",
        "buffer = ExperienceReplay(replay_start_size)\n",
        "environment = foolgame.Environment(players_num,\n",
        "                                   env_type=\"dummy\",\n",
        "                                   observer_player=1,\n",
        "                                   nnmodel=None)\n",
        "game_table = Agent(environment,\n",
        "                   exp_buffer=buffer)\n",
        "\n",
        "gamma = 0.99\n",
        "tau = 0.00005                                   \n",
        "batch_size = 128                   \n",
        "\n",
        "eps_start = 1.0\n",
        "eps_decay = .999985             #changed start from .999985\n",
        "eps_min = 0.07                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOv0uPpC1ep3"
      },
      "source": [
        "model_weights_save_name = 'fool_cardgame_weights'\n",
        "history_csv_name = 'fool_cardgame_hst.csv'\n",
        "exp_pkl_name = 'fool_cardgame_exp.pkl'\n",
        "cols = ['game_episode',\n",
        "        'turns',  \n",
        "        'time', \n",
        "        'loss',\n",
        "        'epsilon',\n",
        "        'mean_reward', \n",
        "       ]\n",
        "\n",
        "def get_pd_data(pdfile):\n",
        "  df_data = pd.read_csv(os.path.join(HOME, pdfile), index_col=0)\n",
        "  return df_data\n",
        "\n",
        "def add_data_to_pd(df_data, _game_episode, _turns, _time, _loss, _epsilon, _mean_reward):\n",
        "  df_data = df_data.append({'game_episode': _game_episode,\n",
        "                            'turns': _turns,\n",
        "                            'time': _time,\n",
        "                            'loss': _loss,\n",
        "                            'epsilon': _epsilon,\n",
        "                            'mean_reward': _mean_reward,\n",
        "                            }, ignore_index=True)\n",
        "  # print(self.df.to_string())\n",
        "  return df_data\n",
        "  \n",
        "def load_checkpoint(save_name):\n",
        "    pd_file_path =  os.path.join(HOME, history_csv_name)\n",
        "    if os.path.exists(pd_file_path):    \n",
        "      df = get_pd_data(pd_file_path)\n",
        "      pd_start_game= int(df.game_episode.max())\n",
        "    else:\n",
        "      pd_start_game = 0\n",
        "      df = pd.DataFrame(columns=cols)\n",
        "    start_game = 0\n",
        "    mean_reward = 0\n",
        "    rewards_in_row = []\n",
        "    dirlist = os.listdir(HOME)\n",
        "    for i in range (len(dirlist)):\n",
        "      filename = dirlist[i]\n",
        "      if save_name in filename:\n",
        "        if '_' in filename: \n",
        "          try: \n",
        "            epoch = int((filename.split('_')[-1]).split('.')[-2])\n",
        "          except ValueError:\n",
        "            epoch = 0\n",
        "          if epoch > start_game:\n",
        "            start_game = epoch\n",
        "    if pd_start_game > start_game:\n",
        "      df.drop(index = [i for i in range(start_game+1, pd_start_game+1)], axis=0, inplace=True)\n",
        "    if start_game > 0:\n",
        "      file_path = os.path.join(HOME, f'{save_name}_{start_game}.h5')\n",
        "      model.load_state_dict(torch.load(file_path))\n",
        "      target_model.load_state_dict(torch.load(file_path))\n",
        "\n",
        "      print(f\"Loaded {file_path} save, for model & model_target. Starting training from {start_game}\")\n",
        "      # last epoch +1\n",
        "      epsilon = float(df.loc[(df[\"game_episode\"]==start_game), \"epsilon\"].item())\n",
        "      mean_reward = float(df.loc[(df[\"game_episode\"]==start_game), \"mean_reward\"].item())\n",
        "    else:\n",
        "      epsilon = 1.0\n",
        "      print(f\"Starting from scratches. Starting game = {start_game}\")\n",
        "      rewards_in_row = []\n",
        "    return df, start_game, epsilon, mean_reward, rewards_in_row\n",
        "\n",
        "df, start_game, epsilon, mean_reward, rewards_in_row = load_checkpoint(model_weights_save_name)\n",
        "\n",
        "if start_game != 0:\n",
        "  turn_idx = int(df.turns.max())\n",
        "  figshow(df)\n",
        "  game_episode = start_game\n",
        "  total_rewards = []\n",
        "  total_rewards = df.loc[(df[\"game_episode\"]<=start_game) &(df[\"game_episode\"]>=start_game-100), \"mean_reward\"].values.tolist()\n",
        "  # epsilon = 0.1\n",
        "  learning_rate = 0.00025\n",
        "  adam_hat = 1e-8\n",
        "  game_time = 0\n",
        "  if game_episode > 200:\n",
        "    lr_decay = 200/game_episode\n",
        "    learning_rate = learning_rate * lr_decay\n",
        "    if learning_rate < adam_hat:\n",
        "      learning_rate = adam_hat\n",
        "else:\n",
        "  turn_idx = 0\n",
        "  mean_reward = 0\n",
        "  game_episode = -1\n",
        "  epsilon = eps_start\n",
        "  total_rewards = []\n",
        "  learning_rate = 0.00025 \n",
        "\n",
        "print(df.tail(15).to_string())\n",
        "print(f'Start game episode:', start_game)\n",
        "print('Turn index:', turn_idx)\n",
        "print('Epsilon:', epsilon)\n",
        "print('Learning rate:', learning_rate)\n",
        "print('Mean reward on last game episode:', mean_reward)\n",
        "print(f'_mean_ of Total rewards for last 100 games: {np.mean(total_rewards[-100:]) if total_rewards else None}')\n",
        "\n",
        "exp_file_path = os.path.join(HOME, f'{exp_pkl_name}')\n",
        "if start_game != 0:\n",
        "  if os.path.exists(exp_file_path):\n",
        "    buffer.load(exp_file_path)\n",
        "  else:\n",
        "    msg = f\"Warning: pickle Exp file not found, starting without\" \n",
        "    print(msg)\n",
        "\n",
        "time_sum = int(df[\"time\"].sum())\n",
        "print(f'Model learning {start_game} games and {timedelta(seconds=time_sum)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nco3OLdg1ep5"
      },
      "source": [
        "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "msg_display = True\n",
        "best_mean_reward = None\n",
        "num_actions = 37\n",
        "loss_numpy = 0\n",
        "loss = 0\n",
        "start_datetime = datetime.now(timezone)\n",
        "print(\">>>Training starts at \", start_datetime)\n",
        "start_time = time.time()\n",
        "episodes_start_time = time.time()\n",
        "\n",
        "while True:\n",
        "        turn_idx += 1\n",
        "        if len(buffer) < replay_start_size:\n",
        "            epsilon = 1.0\n",
        "        reward = game_table.play_step(model, epsilon=epsilon, step_device=device)\n",
        "        if reward is not None:\n",
        "            epsilon = max(epsilon*eps_decay, eps_min)\n",
        "            reward = np.float16(reward)\n",
        "            game_episode += 1\n",
        "            total_rewards.append(reward)\n",
        "            if len(total_rewards) > 100:\n",
        "              mean_reward = np.mean(total_rewards[-100:])\n",
        "              del total_rewards[:1]\n",
        "            msg = f\"\\rgame_episode: {game_episode:07d}, turn: {turn_idx:09d}, epsilon: {epsilon:.3f}, \" \\\n",
        "                  f\"mean reward: {mean_reward:.3f}, loss: {loss_numpy:.4f}, \" \\\n",
        "                  f\"last reward {reward}\"\n",
        "\n",
        "            if (best_mean_reward is None or best_mean_reward < mean_reward) \\\n",
        "              and game_episode > 1000:\n",
        "                best_mean_reward = mean_reward\n",
        "                if best_mean_reward is not None:\n",
        "                    torch.save(model.state_dict(), os.path.join(HOME, f'{model_weights_save_name}_{game_episode:002d}.h5'))\n",
        "                    msg = f\"\\rgame_episode: {game_episode:07d}, turn: {turn_idx:09d}, epsilon: {epsilon:.3f}, \" \\\n",
        "                    f\"mean reward: {mean_reward:.3f}, loss: {loss_numpy:.4f}, \" \\\n",
        "                    f\"last reward {reward}\"\n",
        "                    msg = f'{msg}. Best mean reward updated {best_mean_reward:.3f}'\n",
        "                    msg_display = True\n",
        "\n",
        "            if mean_reward > MEAN_REWARD_BOUND \\\n",
        "              and len(buffer) > replay_start_size \\\n",
        "              and epsilon == 0.02:\n",
        "                end_time = time.time()\n",
        "                game_time = end_time - start_time \n",
        "                start_time  = time.time()\n",
        "                print(f'{msg}, time: {game_time:.4f} sec')    \n",
        "                print(\"Solved in %d turns!\" % turn_idx)\n",
        "                loss_numpy = loss_t.item()\n",
        "                # if tf.is_tensor(loss):\n",
        "                    # loss_numpy = tf.keras.backend.get_value(loss)\n",
        "                df = add_data_to_pd(df, game_episode, turn_idx, game_time, loss_numpy, epsilon, mean_reward)\n",
        "                df.to_csv(os.path.join(HOME, history_csv_name), encoding='utf-8')\n",
        "                break\n",
        "            end_time = time.time()\n",
        "            game_time = end_time - start_time \n",
        "            start_time  = time.time()\n",
        "\n",
        "            # if tf.is_tensor(loss):\n",
        "                # loss_numpy = tf.keras.backend.get_value(loss)    \n",
        "            if game_episode % 100 == 0 or msg_display:\n",
        "              episodes_end_time = time.time()\n",
        "              episodes_time = episodes_end_time - episodes_start_time \n",
        "              episodes_start_time  = time.time() \n",
        "              print(f'{msg}, episodes time: {episodes_time:.4f} sec')\n",
        "              msg_display = False\n",
        "            df = add_data_to_pd(df, game_episode, turn_idx, game_time, loss_numpy, epsilon, mean_reward)\n",
        "            if (game_episode % 500 == 0) and (game_episode != 0):\n",
        "                df.to_csv(os.path.join(HOME, history_csv_name), encoding='utf-8')\n",
        "            if (game_episode % 2500 == 0) and (game_episode != 0) and (turn_idx >= replay_start_size):\n",
        "                print('Saving weights...')\n",
        "                torch.save(model.state_dict(), os.path.join(HOME, f'{model_weights_save_name}_{game_episode:002d}.h5'))\n",
        "                buffer.save(exp_file_path, buffer_length=50000)\n",
        "            if game_episode % 1000 == 0:\n",
        "                figshow(df)\n",
        "            if (game_episode % 10000 == 0) and (game_episode != 0):  \n",
        "                clear_output()\n",
        "                print(df.tail(50).to_string())\n",
        "                figshow(df)\n",
        "        if len(buffer) < replay_start_size:\n",
        "            continue\n",
        "\n",
        "        batch = buffer.sample(batch_size)\n",
        "        states, actions, valid_actions_lst, rewards, dones, next_states = batch\n",
        "\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.LongTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        dones = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "        # resize tensors\n",
        "        actions = actions.view(actions.size(0), 1)\n",
        "        dones = dones.view(dones.size(0), 1)\n",
        "\n",
        "        # compute loss\n",
        "        curr_Q = model.forward(states).gather(1, actions)\n",
        "        next_Q = target_model.forward(next_states)\n",
        "        max_next_Q = torch.max(next_Q, 1)[0]\n",
        "        max_next_Q = max_next_Q.view(max_next_Q.size(0), 1)\n",
        "        # max_next_Q = max_next_Q.to(device)\n",
        "        expected_Q = rewards + (1 - dones) * gamma * max_next_Q\n",
        "        # expected_Q = expected_Q.to(device)\n",
        "\n",
        "        loss = F.mse_loss(curr_Q, expected_Q.detach())\n",
        "        # loss = nn.MSELoss()(curr_Q, expected_Q.detach)\n",
        "\n",
        "        # states_v = torch.tensor(states).to(device)\n",
        "        # next_states_v = torch.tensor(next_states).to(device)\n",
        "        # actions_v = torch.tensor(actions).to(device)\n",
        "        # rewards_v = torch.tensor(rewards).to(device)\n",
        "        # done_mask = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "        # state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # next_state_values = target_net(next_states_v).max(1)[0]\n",
        "\n",
        "        # next_state_values[done_mask] = -1.0\n",
        "\n",
        "        # next_state_values = next_state_values.detach()\n",
        "\n",
        "        # expected_state_action_values = next_state_values * gamma + rewards_v\n",
        "\n",
        "        # loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_numpy = loss.item()\n",
        "        \n",
        "        # target network update\n",
        "        if turn_idx % 2 == 0:\n",
        "          for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
        "              target_param.data.copy_(tau * param + (1 - tau) * target_param)\n",
        "\n",
        "        # if turn_idx % sync_target_turns == 0:\n",
        "            # update the the target network with new weights\n",
        "            # print(\"Synchronizing models...\")\n",
        "            # target_net.load_state_dict(net.state_dict())\n",
        "            # model_target.set_weights(model.get_weights())\n",
        "        # for target_param, param in zip(target_net.parameters(), net.parameters()):\n",
        "        #     target_param.data.copy_(tau * param + (1 - tau) * target_param)\n",
        "\n",
        "print('Saving weights...')\n",
        "torch.save(model1.state_dict(), os.path.join(HOME, f'{model_weights_save_name}_{game_episode:002d}.h5'))\n",
        "buffer.save(exp_file_path)\n",
        "figshow(df)\n",
        "\n",
        "end_datetime = datetime.now(timezone)\n",
        "print(\">>>Training ends at \", end_datetime)\n",
        "train_time = end_datetime-start_datetime\n",
        "print(f'Model trained:', train_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHsCAAc8T74g"
      },
      "source": [
        ""
      ]
    }
  ]
}