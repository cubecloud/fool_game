{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fool_game_and_TD3_env_steps_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AUNYbmtaU8Fm",
        "XKOLyrAVhMwX",
        "XVB66D2S1epw"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cubecloud/fool_game/blob/feature-07-td3-refactoring/fool_game_and_TD3_env_steps_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkKWFvKdjz81"
      },
      "source": [
        "### New Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrsJc8Gx6nvo",
        "outputId": "4b4fe198-841f-414c-ccad-79677d960ea9"
      },
      "source": [
        "!pip install git+https://github.com/cubecloud/fool_game.git@feature-07-td3-refactoring"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/cubecloud/fool_game.git@feature-07-td3-refactoring\n",
            "  Cloning https://github.com/cubecloud/fool_game.git (to revision feature-07-td3-refactoring) to /tmp/pip-req-build-2guw24t_\n",
            "  Running command git clone -q https://github.com/cubecloud/fool_game.git /tmp/pip-req-build-2guw24t_\n",
            "  Running command git checkout -b feature-07-td3-refactoring --track origin/feature-07-td3-refactoring\n",
            "  Switched to a new branch 'feature-07-td3-refactoring'\n",
            "  Branch 'feature-07-td3-refactoring' set up to track remote branch 'feature-07-td3-refactoring' from 'origin'.\n",
            "Requirement already satisfied: setuptools>=51.0.0 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.64) (56.1.0)\n",
            "Collecting tensorboardx>=2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.64) (2018.9)\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.64) (3.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.64) (0.11.1)\n",
            "Collecting dataclasses>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from fool-game==0.2.64) (1.1.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx>=2.2->fool-game==0.2.64) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx>=2.2->fool-game==0.2.64) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->fool-game==0.2.64) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->fool-game==0.2.64) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->fool-game==0.2.64) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->fool-game==0.2.64) (2.4.7)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.11.1->fool-game==0.2.64) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx>=2.2->fool-game==0.2.64) (1.15.0)\n",
            "Building wheels for collected packages: fool-game\n",
            "  Building wheel for fool-game (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fool-game: filename=fool_game-0.2.64-cp37-none-any.whl size=29209 sha256=ac83340026f0bca123be90d04eae5dca8d99b209b6b5d47a073aa36a849ef034\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jibse0lc/wheels/38/c2/4e/28e56445c7bf68d7fa9b8fd53058209ed98fe03bf84a54a441\n",
            "Successfully built fool-game\n",
            "Installing collected packages: tensorboardx, dataclasses, fool-game\n",
            "Successfully installed dataclasses-0.6 fool-game-0.2.64 tensorboardx-2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acpTAjrvSGZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b780b5d2-1ae1-40ee-be54-965faa3295aa"
      },
      "source": [
        "# adding terra_ai location for using googlesync \n",
        "# for development on local drive and testing \n",
        "# in google colab or jupyter notebook \n",
        "# change this variables to your locations for development\n",
        "local_drive = '/home/cubecloud/GDrive'\n",
        "remote_drive = '/content/drive/MyDrive'\n",
        "local_dev = '/Python/fool_game/'\n",
        "remote_dev = '/Python/fool_game/'\n",
        "__demo_version__ = \"0.1.0\"\n",
        "import sys\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/drive')\n",
        "    DEV = f'{remote_drive}{remote_dev}'\n",
        "    DRIVE = remote_drive\n",
        "elif 'ipykernel' in str(get_ipython()):\n",
        "    print('Running on Jupyter Notebook')\n",
        "    DEV = f'{local_drive}{local_dev}'\n",
        "    DRIVE = local_drive\n",
        "else:\n",
        "    sys.exit('Not running on CoLab or Jupyter notebook')\n",
        "print(f'Adding sys path: {DEV}')\n",
        "sys.path.append(DEV)\n",
        "HOME = f'{DEV}data/'\n",
        "\n",
        "#check environment \n",
        "\n",
        "import tensorflow\n",
        "print('Checking key environment depenndecies')\n",
        "!python --version\n",
        "print('TensorFlow', tensorflow.__version__)\n",
        "print('Keras', tensorflow.keras.__version__)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n",
            "Adding sys path: /content/drive/MyDrive/Python/fool_game/\n",
            "Checking key environment depenndecies\n",
            "Python 3.7.10\n",
            "TensorFlow 2.4.1\n",
            "Keras 2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J1alVgtUgUf",
        "outputId": "f3cd1c3d-f0dd-4431-c153-1ba3a1bdaf5b"
      },
      "source": [
        "import collections\n",
        "import numpy as np # импортируем библиотеку для работы с массивами данных\n",
        "import tensorflow as tf\n",
        "# from tensorflow.keras.models import Model, load_model \n",
        "# from tensorflow.keras import layers\n",
        "# from tensorflow.keras.layers import Dense, Flatten, Input, Lambda, Conv2D, MaxPooling2D, Reshape, Multiply # из кераса загружаем необходимые слои для нейросети\n",
        "# from tensorflow.keras.layers import BatchNormalization\n",
        "# from tensorflow.keras.optimizers import RMSprop, Adam, SGD, RMSprop# из кераса загружаем выбранный оптимизатор\n",
        "import time                                # модуль для операций со временными характеристиками\n",
        "import matplotlib.pyplot as plt            # импортируем библиотеку для визуализации данных\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import clear_output \n",
        "\n",
        "import pytz\n",
        "timezone = pytz.timezone(\"Europe/Moscow\")\n",
        "# # \"магическая\" команда python для запуска библиотеки в ноутбуке\n",
        "# %matplotlib inline\n",
        "\n",
        "# HOME = f'/content/drive/MyDrive/Python/fool_game/data/'\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "from importlib import reload\n",
        "from cardgames import foolgame\n",
        "foolgame = reload(foolgame)\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)\n",
        "print(foolgame.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "2.4.0\n",
            "0.02.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUNYbmtaU8Fm"
      },
      "source": [
        "### Figshow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_qf7NMJwVjs"
      },
      "source": [
        "def figshow(df):\n",
        "    fig = plt.figure(figsize=(26, 7))\n",
        "    sns.set_style(\"white\")\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.set_axisbelow(True)\n",
        "    ax1.minorticks_on()\n",
        "    ax1.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "    ax1.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "    last_game = int(df[\"game_episode\"].max())\n",
        "    # N = np.arange(0, last_game+1)\n",
        "    n_games = 1\n",
        "    if last_game // 400 > 0:\n",
        "        n_games = last_game // 400\n",
        "    \n",
        "\n",
        "    plt.plot(df.loc[(df[\"game_episode\"] % n_games == 0), \"game_episode\"], \n",
        "             df.loc[(df[\"game_episode\"] % n_games == 0), \"loss\"], \n",
        "             linestyle='--', color='blue', label=\"loss\")\n",
        "    plt.plot(df.loc[(df[\"game_episode\"] % n_games == 0), \"game_episode\"], \n",
        "             df.loc[(df[\"game_episode\"] % n_games == 0), \"epsilon\"], \n",
        "             linestyle='--', color='green', label=\"epsilon\")\n",
        "    plt.title(f\"Loss & epsilon\")\n",
        "    plt.legend()\n",
        "\n",
        "    ax2 = fig.add_subplot(1, 2, 2)\n",
        "    ax2.set_axisbelow(True)\n",
        "    ax2.minorticks_on()\n",
        "    ax2.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "    ax2.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "    plt.plot(df.loc[(df[\"game_episode\"] % n_games == 0), \"game_episode\"],\n",
        "             df.loc[(df[\"game_episode\"] % n_games == 0), 'mean_reward'], \n",
        "             linestyle='-', color='red', label=\"mean_reward\")\n",
        "    plt.title(f\"mean_reward\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    pass"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7uAU9CHLGaa",
        "outputId": "be279c1a-e1b4-44af-febd-61f0a716f51a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue May 25 16:21:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wznv9I1KR_I3"
      },
      "source": [
        "## The DQN model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6B8v-Qh5Ykk"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn        # Pytorch neural network package\n",
        "import torch.optim as optim  # Pytorch optimization package\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "# device = torch.device(\"cuda\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqcYBObzuAvg"
      },
      "source": [
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"Initialize parameters and build model.\n",
        "        Args:\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            max_action (float): highest action to take\n",
        "            seed (int): Random seed\n",
        "            h1_units (int): Number of nodes in first hidden layer\n",
        "            h2_units (int): Number of nodes in second hidden layer\n",
        "            \n",
        "        Return:\n",
        "            action output of network with tanh activation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.max_action * torch.tanh(self.l3(x)) \n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twM4nrgJtrJB"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    \"\"\"Initialize parameters and build model.\n",
        "        Args:\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            max_action (float): highest action to take\n",
        "            seed (int): Random seed\n",
        "            h1_units (int): Number of nodes in first hidden layer\n",
        "            h2_units (int): Number of nodes in second hidden layer\n",
        "            \n",
        "        Return:\n",
        "            value output of network \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Q1 architecture\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "\n",
        "        # Q2 architecture\n",
        "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l5 = nn.Linear(400, 300)\n",
        "        self.l6 = nn.Linear(300, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "\n",
        "        x1 = F.relu(self.l1(xu))\n",
        "        x1 = F.relu(self.l2(x1))\n",
        "        x1 = self.l3(x1)\n",
        "\n",
        "        x2 = F.relu(self.l4(xu))\n",
        "        x2 = F.relu(self.l5(x2))\n",
        "        x2 = self.l6(x2)\n",
        "        return x1, x2\n",
        "\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "\n",
        "        x1 = F.relu(self.l1(xu))\n",
        "        x1 = F.relu(self.l2(x1))\n",
        "        x1 = self.l3(x1)\n",
        "        return x1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG-NB0oDiFEg"
      },
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0nn9NHw1Qa5"
      },
      "source": [
        "\n",
        "\n",
        "# Code based on: \n",
        "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
        "\n",
        "# Expects tuples of (state, next_state, action, reward, done)\n",
        "class ReplayBuffer(object):\n",
        "    \"\"\"Buffer to store tuples of experience replay\"\"\"\n",
        "    \n",
        "    def __init__(self, max_size=1000000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_size (int): total amount of tuples to store\n",
        "        \"\"\"\n",
        "        \n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "\n",
        "    def add(self, data):\n",
        "        \"\"\"Add experience tuples to buffer\n",
        "        \n",
        "        Args:\n",
        "            data (tuple): experience replay tuple\n",
        "        \"\"\"\n",
        "        \n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = data\n",
        "            self.ptr = (self.ptr + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(data)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samples a random amount of experiences from buffer of batch size\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int): size of sample\n",
        "        \"\"\"\n",
        "        \n",
        "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        states, actions, next_states, rewards, dones = [], [], [], [], []\n",
        "\n",
        "        for i in ind: \n",
        "            s, a, s_, r, d = self.storage[i]\n",
        "            states.append(np.array(s, copy=False))\n",
        "            actions.append(np.array(a, copy=False))\n",
        "            next_states.append(np.array(s_, copy=False))\n",
        "            rewards.append(np.array(r, copy=False))\n",
        "            dones.append(np.array(d, copy=False))\n",
        "\n",
        "        return np.array(states), np.array(actions), np.array(next_states), np.array(rewards).reshape(-1, 1), np.array(dones).reshape(-1, 1)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKOLyrAVhMwX"
      },
      "source": [
        "#### old experience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obhi7_P2RSpQ"
      },
      "source": [
        "# Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'valid_action_lst','reward', 'done', 'next_state'])\n",
        "\n",
        "# class ExperienceReplay:\n",
        "#     def __init__(self, capacity):\n",
        "#         self.capacity = capacity\n",
        "#         self.buffer = collections.deque(maxlen=capacity)\n",
        "#         pass\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.buffer)\n",
        "\n",
        "#     def get_length(self):\n",
        "#         return self.__len__()\n",
        "\n",
        "#     def append(self, experience):\n",
        "#         self.buffer.append(experience)\n",
        "#         pass\n",
        "\n",
        "#     def sample(self, batch_size):\n",
        "#         indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "#         states, actions, valid_actions_lst, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "#         return (np.array(states, dtype=np.float32), \n",
        "#                 np.array(actions, dtype=np.int64), \n",
        "#                 np.array(valid_actions_lst), \n",
        "#                 np.array(rewards, dtype=np.float32).reshape(-1, 1),\n",
        "#                 np.array(dones, dtype=np.uint8).reshape(-1, 1), \n",
        "#                 np.array(next_states, dtype=np.float32))\n",
        "\n",
        "#     def save(self, file_path, buffer_length=10000):\n",
        "#         len_buffer = len(self.buffer)\n",
        "#         with open(file_path, \"wb\") as f:\n",
        "#             print('Save exp buffer...')\n",
        "#             if not (self.capacity is None) \\\n",
        "#                     and (len_buffer < self.capacity) \\\n",
        "#                     and (len_buffer < buffer_length):\n",
        "#                 buffer_length = len_buffer\n",
        "#             else:\n",
        "#                 buffer_length = len_buffer\n",
        "#             states, actions, valid_actions_lst, rewards, dones, next_states = \\\n",
        "#                 zip(*[self.buffer[idx] for idx in range(len(self.buffer) - buffer_length, len(self.buffer))])\n",
        "#             pkl.dump([states, actions, valid_actions_lst, rewards, dones, next_states], f)\n",
        "#             del [states, actions, valid_actions_lst, rewards, dones, next_states]\n",
        "#             pass\n",
        "\n",
        "#     def load(self, file_path):\n",
        "#         with open(file_path, \"rb\") as f:\n",
        "#             print('Loading exp buffer...')\n",
        "#             # self.buffer = pkl.load(f)\n",
        "#             states, actions, valid_actions_lst, rewards, dones, next_states = pkl.load(f)\n",
        "#             for state, action, valid_action_lst, reward, done, next_state in zip(states, actions, valid_actions_lst, rewards, dones, next_states):\n",
        "#                 exp = Experience(state, action, valid_action_lst, reward, done, next_state)\n",
        "#                 self.buffer.append(exp)\n",
        "#             del [states, actions, valid_actions_lst, rewards, dones, next_states]\n",
        "#         pass"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Z2t-QEhSva"
      },
      "source": [
        "### TD3 Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCx88PAJzTmH"
      },
      "source": [
        "class TD3(object):\n",
        "    \"\"\"Agent class that handles the training of the networks and provides outputs as actions\n",
        "    \n",
        "        Args:\n",
        "            state_dim (int): state size\n",
        "            action_dim (int): action size\n",
        "            max_action (float): highest action to take\n",
        "            device (device): cuda or cpu to process tensors\n",
        "            env (env): gym environment to use\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, max_action, env):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.env = env\n",
        "\n",
        "        \n",
        "    def select_action(self, state, noise=0.1):\n",
        "        \"\"\"Select an appropriate action from the agent policy\n",
        "        \n",
        "            Args:\n",
        "                state (array): current state of environment\n",
        "                noise (float): how much noise to add to acitons\n",
        "                \n",
        "            Returns:\n",
        "                action (float): action clipped within action range\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        \n",
        "        action = self.actor(state).cpu().data.numpy().flatten()\n",
        "        # print('action from actor',action)\n",
        "        if noise != 0: \n",
        "            action = (action + np.random.normal(0, noise, size=action.shape))\n",
        "            # action = (action + np.random.normal(0, noise, size=self.env.action_space.shape[0]))\n",
        "        # print('action before clip', action)\n",
        "        # action = action.clip(self.env.action_space.low, self.env.action_space.high)\n",
        "        action = action.clip(-1.0, 1.0)\n",
        "        # print('action after clip', action)\n",
        "        return action\n",
        "\n",
        "    \n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "        \"\"\"Train and update actor and critic networks\n",
        "        \n",
        "            Args:\n",
        "                replay_buffer (ReplayBuffer): buffer for experience replay\n",
        "                iterations (int): how many times to run training\n",
        "                batch_size(int): batch size to sample from replay buffer\n",
        "                discount (float): discount factor\n",
        "                tau (float): soft update for main networks to target networks\n",
        "                \n",
        "            Return:\n",
        "                actor_loss (float): loss from actor network\n",
        "                critic_loss (float): loss from critic network\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        for it in range(iterations):\n",
        "\n",
        "            # Sample replay buffer \n",
        "            x, y, u, r, d = replay_buffer.sample(batch_size)\n",
        "\n",
        "            state = torch.FloatTensor(x).to(device)\n",
        "            action = torch.FloatTensor(u).to(device)\n",
        "            next_state = torch.FloatTensor(y).to(device)\n",
        "            done = torch.FloatTensor(1 - d).to(device)\n",
        "            reward = torch.FloatTensor(r).to(device)\n",
        "\n",
        "            # Select action according to policy and add clipped noise \n",
        "            noise = torch.FloatTensor(u).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Compute the target Q value\n",
        "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + (done * discount * target_Q).detach()\n",
        "\n",
        "            # Get current Q estimates\n",
        "            current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "            # Compute critic loss\n",
        "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q) \n",
        "\n",
        "            # Optimize the critic\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            # Delayed policy updates\n",
        "            if it % policy_freq == 0:\n",
        "\n",
        "                # Compute actor loss\n",
        "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "                # Optimize the actor \n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # Update the frozen target models\n",
        "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "\n",
        "    def load(self, filename=\"best_avg\", directory=\"./saves\"):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfdza2EyzxCg"
      },
      "source": [
        "class Runner():\n",
        "    \"\"\"Carries out the environment steps and adds experiences to memory\"\"\"\n",
        "    \n",
        "    def __init__(self, env, agent, replay_buffer):\n",
        "        \n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.obs = self.env.reset()\n",
        "        self.done = False\n",
        "        self._reset()\n",
        "        pass\n",
        "\n",
        "    def _reset(self):\n",
        "        self.obs = self.env.reset()\n",
        "        # self.env.step(0, first_step=True)\n",
        "        pass\n",
        "\n",
        "    def next_step(self, episode_timesteps, noise=0.1):\n",
        "        action = self.agent.select_action(np.array(self.obs), noise=0.1)\n",
        "\n",
        "        if not self.env.action_space.is_this_valid_action_ohe(action):\n",
        "            reward = -0.007\n",
        "            done_bool = False\n",
        "            replay_buffer.add((self.obs, self.obs, action, reward, done_bool))\n",
        "            action = self.env.action_space.sample_ohe()\n",
        "            # print(action)\n",
        "        \n",
        "        # Perform action\n",
        "        new_obs, reward, done, info = self.env.step(action) \n",
        "        done_bool = 0 if episode_timesteps + 1 == 200 else float(done)\n",
        "        # print(info)\n",
        "        # Store data in replay buffer\n",
        "        replay_buffer.add((self.obs, new_obs, action, reward, done_bool))\n",
        "        \n",
        "        self.obs = new_obs\n",
        "        \n",
        "        if done:\n",
        "            # self.obs = self.env.reset()\n",
        "            done = False\n",
        "            # print('Done')\n",
        "            self._reset()\n",
        "            return reward, True\n",
        "        \n",
        "        return reward, done"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vF_vMKRz4NV"
      },
      "source": [
        "def evaluate_policy(policy, env, eval_episodes=100,render=False):\n",
        "    \"\"\"run several episodes using the best agent policy\n",
        "        \n",
        "        Args:\n",
        "            policy (agent): agent to evaluate\n",
        "            env (env): gym environment\n",
        "            eval_episodes (int): how many test episodes to run\n",
        "            render (bool): show training\n",
        "        \n",
        "        Returns:\n",
        "            avg_reward (float): average reward over the number of evaluations\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    avg_reward = 0.\n",
        "    for i in range(eval_episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if render:\n",
        "                env.render()\n",
        "            action = policy.select_action(np.array(obs), noise=0)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            avg_reward += reward\n",
        "\n",
        "    avg_reward /= eval_episodes\n",
        "\n",
        "    print(\"\\n---------------------------------------\")\n",
        "    print(\"Evaluation over {:d} episodes: {:f}\" .format(eval_episodes, avg_reward))\n",
        "    print(\"---------------------------------------\")\n",
        "    return avg_reward\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-F_ds0V0J7-"
      },
      "source": [
        "def observe(env,replay_buffer, observation_steps):\n",
        "    \"\"\"run episodes while taking random actions and filling replay_buffer\n",
        "    \n",
        "        Args:\n",
        "            env (env): gym environment\n",
        "            replay_buffer(ReplayBuffer): buffer to store experience replay\n",
        "            observation_steps (int): how many steps to observe for\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    time_steps = 0\n",
        "    obs = env.reset()\n",
        "    # env.step(0, first_step=True)\n",
        "    # obs = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while time_steps < observation_steps:\n",
        "        action = env.action_space.sample_ohe()\n",
        "        if not env.action_space.is_this_valid_action_ohe(action):\n",
        "            reward = -0.007\n",
        "            done_bool = False\n",
        "            replay_buffer.add((obs, obs, action, reward, done_bool))\n",
        "            action = env.action_space.sample_ohe()\n",
        "        # print('Valid actions\\n', env.action_space.pl.analyze())\n",
        "        # print('Action from sample\\n', action)\n",
        "        # print('Desktop\\n', env.action_space.pl.desktop_list)\n",
        "        # print('Cards on hand\\n', env.action_space.pl.player_cards_onhand_list)\n",
        "        new_obs, reward, done, info = env.step(action)\n",
        "        # print(info)\n",
        "        replay_buffer.add((obs, new_obs, action, reward, done))\n",
        "        # assert new_obs.shape == obs.shape, f'Error in states dimensions {new_obs.shape} != {obs.shape}'\n",
        "        obs = new_obs\n",
        "        time_steps += 1\n",
        "\n",
        "        if done:\n",
        "            obs = env.reset()\n",
        "            # env.step(0, first_step=True)\n",
        "            done = False\n",
        "\n",
        "        print(\"\\rPopulating Buffer {}/{}.\".format(time_steps, observation_steps), end=\"\")\n",
        "        sys.stdout.flush()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TD7RrsF0ORL"
      },
      "source": [
        "\n",
        "def train(agent, test_env):\n",
        "    \"\"\"Train the agent for exploration steps\n",
        "    \n",
        "        Args:\n",
        "            agent (Agent): agent to use\n",
        "            env (environment): gym environment\n",
        "            writer (SummaryWriter): tensorboard writer\n",
        "            exploration (int): how many training steps to run\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    total_timesteps = 0\n",
        "    timesteps_since_eval = 0\n",
        "    episode_num = 0\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    done = False \n",
        "    obs = env.reset()\n",
        "    evaluations = []\n",
        "    rewards = []\n",
        "    best_avg = -2000\n",
        "    \n",
        "    writer = SummaryWriter(comment=\"-TD3_foolgame_state_v006\")\n",
        "    \n",
        "    while total_timesteps < EXPLORATION:\n",
        "    \n",
        "        if done: \n",
        "\n",
        "            if total_timesteps != 0: \n",
        "                rewards.append(episode_reward)\n",
        "                avg_reward = np.mean(rewards[-100:])\n",
        "                \n",
        "                writer.add_scalar(\"avg_reward\", avg_reward, total_timesteps)\n",
        "                writer.add_scalar(\"reward_step\", reward, total_timesteps)\n",
        "                writer.add_scalar(\"episode_reward\", episode_reward, total_timesteps)\n",
        "                \n",
        "                if best_avg < avg_reward and len(rewards)>100:\n",
        "                    best_avg = avg_reward\n",
        "                    print(f\"saving best model... Best average = {best_avg:.4f} at {datetime.now(timezone)}\\n\")\n",
        "                    agent.save(\"best_avg\",\"saves\")\n",
        "\n",
        "                print(\"\\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f} \".format(\n",
        "                    total_timesteps, episode_num, episode_reward, avg_reward), end=\"\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "\n",
        "                if avg_reward >= REWARD_THRESH:\n",
        "                    break\n",
        "\n",
        "                agent.train(replay_buffer, episode_timesteps, BATCH_SIZE, GAMMA, TAU, NOISE, NOISE_CLIP, POLICY_FREQUENCY)\n",
        "\n",
        "                # Evaluate episode\n",
        "#                 if timesteps_since_eval >= EVAL_FREQUENCY:\n",
        "#                     timesteps_since_eval %= EVAL_FREQUENCY\n",
        "#                     eval_reward = evaluate_policy(agent, test_env)\n",
        "#                     evaluations.append(avg_reward)\n",
        "#                     writer.add_scalar(\"eval_reward\", eval_reward, total_timesteps)\n",
        "\n",
        "#                     if best_avg < eval_reward:\n",
        "#                         best_avg = eval_reward\n",
        "#                         print(\"saving best model....\\n\")\n",
        "#                         agent.save(\"best_avg\",\"saves\")\n",
        "\n",
        "                episode_reward = 0\n",
        "                episode_timesteps = 0\n",
        "                episode_num += 1 \n",
        "\n",
        "        reward, done = runner.next_step(episode_timesteps)\n",
        "        episode_reward += reward\n",
        "\n",
        "        episode_timesteps += 1\n",
        "        total_timesteps += 1\n",
        "        timesteps_since_eval += 1\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgbjZZHOa9Y7"
      },
      "source": [
        "# ENV = \"RoboschoolHalfCheetah-v1\"#\"Pendulum-v0\"\n",
        "SEED = 0\n",
        "OBSERVATION = 10000\n",
        "EXPLORATION = 5000000\n",
        "BATCH_SIZE = 100\n",
        "GAMMA = 0.99\n",
        "TAU = 0.005\n",
        "NOISE = 0.2\n",
        "NOISE_CLIP = 0.5\n",
        "EXPLORE_NOISE = 0.1\n",
        "POLICY_FREQUENCY = 2\n",
        "EVAL_FREQUENCY = 5000\n",
        "REWARD_THRESH = 8000"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQOZPszNbDz5"
      },
      "source": [
        "# env = gym.make(ENV)\n",
        "players_num = 2\n",
        "env = foolgame.Environment(players_num,\n",
        "                           env_type=\"Dummy\",\n",
        "                           observer_player=1,\n",
        "                           nnmodel=None)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set seeds\n",
        "# env.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# state_dim = env.observation_space.shape[0]\n",
        "state_dim = 518\n",
        "action_dim = env.action_space.shape[0] \n",
        "# max_action = float(env.action_space.high[0])\n",
        "max_action = env.action_space.abs_minmax\n",
        "\n",
        "\n",
        "policy = TD3(state_dim, action_dim, max_action, env)\n",
        "\n",
        "replay_buffer = ReplayBuffer()\n",
        "\n",
        "runner = Runner(env, policy, replay_buffer)\n",
        "\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdYrZwZafNrf",
        "outputId": "82d8793b-8084-4750-8b49-7a62c7b4dfdc"
      },
      "source": [
        "# Populate replay buffer\n",
        "observe(env, replay_buffer, OBSERVATION)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating Buffer 10000/10000."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdjFvFDPfSbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985ecd72-6209-4733-aa54-d0d3f5c1a5f8"
      },
      "source": [
        "train(policy, env)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total T: 4454 Episode Num: 99 Reward: 1.142000 Avg Reward: 0.082040saving best model... Best average = 0.06200000000000002 at 2021-05-25 19:22:33.963004+03:00\n",
            "\n",
            "Total T: 4495 Episode Num: 100 Reward: -0.894000 Avg Reward: 0.062000saving best model... Best average = 0.08203 at 2021-05-25 19:22:34.363588+03:00\n",
            "\n",
            "Total T: 5094 Episode Num: 114 Reward: -0.846000 Avg Reward: 0.081630saving best model... Best average = 0.10142 at 2021-05-25 19:22:39.540013+03:00\n",
            "\n",
            "Total T: 5126 Episode Num: 115 Reward: 1.107000 Avg Reward: 0.101420saving best model... Best average = 0.10156999999999998 at 2021-05-25 19:22:39.841361+03:00\n",
            "\n",
            "Total T: 5399 Episode Num: 121 Reward: -0.818000 Avg Reward: 0.081730saving best model... Best average = 0.10201000000000002 at 2021-05-25 19:22:42.238122+03:00\n",
            "\n",
            "Total T: 5495 Episode Num: 123 Reward: -0.904000 Avg Reward: 0.101790saving best model... Best average = 0.10201000000000005 at 2021-05-25 19:22:43.080418+03:00\n",
            "\n",
            "Total T: 5540 Episode Num: 124 Reward: 1.134000 Avg Reward: 0.102010saving best model... Best average = 0.10243000000000006 at 2021-05-25 19:22:43.506986+03:00\n",
            "\n",
            "Total T: 5793 Episode Num: 130 Reward: 1.098000 Avg Reward: 0.101510saving best model... Best average = 0.12237999999999999 at 2021-05-25 19:22:45.752005+03:00\n",
            "\n",
            "Total T: 5851 Episode Num: 131 Reward: 1.181000 Avg Reward: 0.122380saving best model... Best average = 0.12253 at 2021-05-25 19:22:46.284446+03:00\n",
            "\n",
            "Total T: 7551 Episode Num: 169 Reward: 1.168000 Avg Reward: 0.102020saving best model... Best average = 0.12304000000000004 at 2021-05-25 19:23:01.430200+03:00\n",
            "\n",
            "Total T: 8054 Episode Num: 179 Reward: 1.153000 Avg Reward: 0.103860saving best model... Best average = 0.12378000000000006 at 2021-05-25 19:23:05.806833+03:00\n",
            "\n",
            "Total T: 8093 Episode Num: 180 Reward: 1.122000 Avg Reward: 0.123780saving best model... Best average = 0.14380000000000004 at 2021-05-25 19:23:06.164644+03:00\n",
            "\n",
            "Total T: 8129 Episode Num: 181 Reward: 1.119000 Avg Reward: 0.143800saving best model... Best average = 0.14417000000000005 at 2021-05-25 19:23:06.529523+03:00\n",
            "\n",
            "Total T: 8419 Episode Num: 187 Reward: 1.133000 Avg Reward: 0.144130saving best model... Best average = 0.14545000000000005 at 2021-05-25 19:23:09.135666+03:00\n",
            "\n",
            "Total T: 8516 Episode Num: 188 Reward: -0.770000 Avg Reward: 0.145450saving best model... Best average = 0.16580000000000006 at 2021-05-25 19:23:09.917695+03:00\n",
            "\n",
            "Total T: 8566 Episode Num: 189 Reward: 1.131000 Avg Reward: 0.165800saving best model... Best average = 0.18596000000000007 at 2021-05-25 19:23:10.347981+03:00\n",
            "\n",
            "Total T: 8665 Episode Num: 191 Reward: 1.140000 Avg Reward: 0.185840saving best model... Best average = 0.2058300000000001 at 2021-05-25 19:23:11.213796+03:00\n",
            "\n",
            "Total T: 8761 Episode Num: 193 Reward: -0.885000 Avg Reward: 0.185820saving best model... Best average = 0.20639000000000007 at 2021-05-25 19:23:12.086284+03:00\n",
            "\n",
            "Total T: 8803 Episode Num: 194 Reward: 1.145000 Avg Reward: 0.206390saving best model... Best average = 0.20683000000000004 at 2021-05-25 19:23:12.467504+03:00\n",
            "\n",
            "Total T: 8894 Episode Num: 196 Reward: 1.116000 Avg Reward: 0.206620saving best model... Best average = 0.2273100000000001 at 2021-05-25 19:23:13.271633+03:00\n",
            "\n",
            "Total T: 8939 Episode Num: 197 Reward: 1.146000 Avg Reward: 0.227310saving best model... Best average = 0.24673000000000006 at 2021-05-25 19:23:13.687266+03:00\n",
            "\n",
            "Total T: 24950 Episode Num: 553 Reward: 1.148000 Avg Reward: 0.228760saving best model... Best average = 0.24909000000000003 at 2021-05-25 19:25:34.494623+03:00\n",
            "\n",
            "Total T: 25003 Episode Num: 554 Reward: 1.162000 Avg Reward: 0.249090saving best model... Best average = 0.24933000000000002 at 2021-05-25 19:25:34.980491+03:00\n",
            "\n",
            "Total T: 25158 Episode Num: 557 Reward: 1.139000 Avg Reward: 0.249060saving best model... Best average = 0.2693900000000001 at 2021-05-25 19:25:36.323263+03:00\n",
            "\n",
            "Total T: 38950 Episode Num: 869 Reward: -0.893000 Avg Reward: 0.266240saving best model... Best average = 0.2867100000000001 at 2021-05-25 19:27:37.726462+03:00\n",
            "\n",
            "Total T: 39002 Episode Num: 870 Reward: 1.145000 Avg Reward: 0.286710saving best model... Best average = 0.28690000000000004 at 2021-05-25 19:27:38.206642+03:00\n",
            "\n",
            "Total T: 39218 Episode Num: 875 Reward: 1.105000 Avg Reward: 0.286660saving best model... Best average = 0.2870700000000001 at 2021-05-25 19:27:40.127555+03:00\n",
            "\n",
            "Total T: 39291 Episode Num: 876 Reward: -0.816000 Avg Reward: 0.287070saving best model... Best average = 0.30703 at 2021-05-25 19:27:40.742023+03:00\n",
            "\n",
            "Total T: 39340 Episode Num: 877 Reward: 1.130000 Avg Reward: 0.307030saving best model... Best average = 0.3070900000000001 at 2021-05-25 19:27:41.183713+03:00\n",
            "\n",
            "Total T: 40118 Episode Num: 893 Reward: -0.863000 Avg Reward: 0.288050saving best model... Best average = 0.30811000000000005 at 2021-05-25 19:27:47.961023+03:00\n",
            "\n",
            "Total T: 40166 Episode Num: 894 Reward: 1.149000 Avg Reward: 0.308110saving best model... Best average = 0.30812000000000006 at 2021-05-25 19:27:48.369482+03:00\n",
            "\n",
            "Total T: 41414 Episode Num: 921 Reward: 1.155000 Avg Reward: 0.290290saving best model... Best average = 0.3108500000000001 at 2021-05-25 19:27:59.380920+03:00\n",
            "\n",
            "Total T: 41455 Episode Num: 922 Reward: 1.130000 Avg Reward: 0.310850saving best model... Best average = 0.3316000000000001 at 2021-05-25 19:27:59.763813+03:00\n",
            "\n",
            "Total T: 41689 Episode Num: 927 Reward: 1.128000 Avg Reward: 0.311970saving best model... Best average = 0.3316400000000001 at 2021-05-25 19:28:01.784621+03:00\n",
            "\n",
            "Total T: 45072 Episode Num: 1003 Reward: 1.149000 Avg Reward: 0.325310saving best model... Best average = 0.34526 at 2021-05-25 19:28:31.455994+03:00\n",
            "\n",
            "Total T: 45506 Episode Num: 1013 Reward: 1.153000 Avg Reward: 0.343720saving best model... Best average = 0.36413000000000006 at 2021-05-25 19:28:35.218330+03:00\n",
            "\n",
            "Total T: 51112 Episode Num: 1137 Reward: 1.121000 Avg Reward: 0.345040saving best model... Best average = 0.36547000000000013 at 2021-05-25 19:29:24.565075+03:00\n",
            "\n",
            "Total T: 176883 Episode Num: 3942 Reward: 1.139000 Avg Reward: 0.347920"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b4poO89RXCh"
      },
      "source": [
        "!tensorboard --logdir= runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVB66D2S1epw"
      },
      "source": [
        "# Pytorch *Agent*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swvZuUju1epz"
      },
      "source": [
        "players_num = 2\n",
        "num_actions=37\n",
        "\n",
        "# sync_target_turns = 3000       \n",
        "replay_start_size = 10000       # start replay at turn_idx\n",
        "\n",
        "model = DQN((518,), 37).to(device)\n",
        "target_model = DQN((518,), 37).to(device)\n",
        "\n",
        "# print(next(model1.parameters()).device)\n",
        "# print(next(model2.parameters()).device)\n",
        "# net = DQN((4536), 37).to(device)\n",
        "# target_net = DQN((4536), 37).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OjeqOkw1epx"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, \n",
        "                 env, \n",
        "                 exp_buffer,\n",
        "                 exp_buffer_size = 50000):\n",
        "        self.env = env\n",
        "        self.observer_player = env.observer_player\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.exp_buffer_size = exp_buffer_size\n",
        "        self.verbose = False\n",
        "        self.debug_verbose = 1\n",
        "        self.gamma = 0.99\n",
        "        self._reset()\n",
        "        pass\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = np.reshape(self.env.reset(), 518)\n",
        "        self.total_reward: float = 0.0\n",
        "        self.env.verbose = self.verbose\n",
        "        self.env.debug_verbose = self.debug_verbose\n",
        "        self.env.step(0, first_step=True)\n",
        "        self.turns_counter = 0\n",
        "        pass\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, step_device=\"cpu\"):\n",
        "        done_reward = None\n",
        "        self.turns_counter += 1\n",
        "        self.env.epsilon = epsilon\n",
        "        step_valid_actions = self.env.pl[self.observer_player].analyze()\n",
        "        if np.random.random() < epsilon:\n",
        "            action = random.choice(step_valid_actions)\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(step_device)\n",
        "            q_vals_v = net.forward(state_v)\n",
        "            # action = np.argmax(q_vals_v.cpu().detach().numpy())\n",
        "            # print(next(net.parameters()).device)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "            # state_a = np.array([self.state], copy=False)\n",
        "            # state = torch.FloatTensor(state_a).float().unsqueeze(0).to(device)\n",
        "            # qvals = net.forward(state)\n",
        "            # action = np.argmax(qvals.cpu().detach().numpy())\n",
        "\n",
        "            # state_a = np.array([self.state], copy=False)\n",
        "            # state_tensor = tf.convert_to_tensor(state_a)\n",
        "            # q_values = nnmodel(state_tensor, training=False)\n",
        "            # # with np.printoptions(precision=3, suppress=True):\n",
        "            # #     print(q_values.numpy())\n",
        "            # valid_masks = tf.one_hot(valid_action_list, self.env.num_actions)\n",
        "            # # with np.printoptions(precision=3, suppress=True):\n",
        "            # #     print(masks.numpy())\n",
        "            # valid_q_values = tf.expand_dims(tf.reduce_sum(tf.multiply(q_values, valid_masks), axis=0), 0)\n",
        "            # # with np.printoptions(precision=3, suppress=True):\n",
        "            # #     print(valid_q_values.numpy())\n",
        "            # action = np.argmax(valid_q_values)\n",
        "            # # print(self.action, action_list, action)\n",
        "        if not (action in step_valid_actions):\n",
        "            # action = step_valid_actions[0]\n",
        "            # action = random.choice(step_valid_actions)\n",
        "            ''' keeping the same state for new_state but adding negative reward'''\n",
        "            new_state = np.squeeze(state_a, axis=0)\n",
        "            self.state = np.squeeze(state_a, axis=0)\n",
        "            # print(new_state.shape)\n",
        "            # print(self.state.shape)\n",
        "            ''' negative reward for not valid action '''\n",
        "            reward = -0.007\n",
        "            is_done = False\n",
        "            self.total_reward += reward\n",
        "            exp = Experience(self.state, action, step_valid_actions, reward, is_done, new_state)\n",
        "            self.exp_buffer.append(exp)\n",
        "            action = random.choice(step_valid_actions)\n",
        "        ''' \n",
        "        step_epsilon - setting to 1.0 - totally random action from agent \n",
        "        trying to learn - how to play valid actions (index + zero (pass)) \n",
        "        '''\n",
        "        new_state, reward, is_done, info = self.env.step(action, step_epsilon=1.0)\n",
        "        new_state = np.reshape(new_state, 518)\n",
        "        # if self.verbose:\n",
        "            # print(\"Last action\", action)\n",
        "            # print(info)\n",
        "        self.total_reward += reward\n",
        "        exp = Experience(self.state, action, step_valid_actions, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or self.turns_counter > 100:\n",
        "            done_reward  = self.total_reward\n",
        "            self.verbose = False\n",
        "            self.debug_verbose = 1\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vghJS8ZV1ep0"
      },
      "source": [
        "MEAN_REWARD_BOUND = 0.85         \n",
        "\n",
        "buffer = ExperienceReplay(replay_start_size)\n",
        "environment = foolgame.Environment(players_num,\n",
        "                                   env_type=\"dummy\",\n",
        "                                   observer_player=1,\n",
        "                                   nnmodel=None)\n",
        "game_table = Agent(environment,\n",
        "                   exp_buffer=buffer)\n",
        "\n",
        "gamma = 0.99\n",
        "tau = 0.00005                                   \n",
        "batch_size = 128                   \n",
        "\n",
        "eps_start = 1.0\n",
        "eps_decay = .999985             #changed start from .999985\n",
        "eps_min = 0.07                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOv0uPpC1ep3"
      },
      "source": [
        "model_weights_save_name = 'fool_cardgame_weights'\n",
        "history_csv_name = 'fool_cardgame_hst.csv'\n",
        "exp_pkl_name = 'fool_cardgame_exp.pkl'\n",
        "cols = ['game_episode',\n",
        "        'turns',  \n",
        "        'time', \n",
        "        'loss',\n",
        "        'epsilon',\n",
        "        'mean_reward', \n",
        "       ]\n",
        "\n",
        "def get_pd_data(pdfile):\n",
        "  df_data = pd.read_csv(os.path.join(HOME, pdfile), index_col=0)\n",
        "  return df_data\n",
        "\n",
        "def add_data_to_pd(df_data, _game_episode, _turns, _time, _loss, _epsilon, _mean_reward):\n",
        "  df_data = df_data.append({'game_episode': _game_episode,\n",
        "                            'turns': _turns,\n",
        "                            'time': _time,\n",
        "                            'loss': _loss,\n",
        "                            'epsilon': _epsilon,\n",
        "                            'mean_reward': _mean_reward,\n",
        "                            }, ignore_index=True)\n",
        "  # print(self.df.to_string())\n",
        "  return df_data\n",
        "  \n",
        "def load_checkpoint(save_name):\n",
        "    pd_file_path =  os.path.join(HOME, history_csv_name)\n",
        "    if os.path.exists(pd_file_path):    \n",
        "      df = get_pd_data(pd_file_path)\n",
        "      pd_start_game= int(df.game_episode.max())\n",
        "    else:\n",
        "      pd_start_game = 0\n",
        "      df = pd.DataFrame(columns=cols)\n",
        "    start_game = 0\n",
        "    mean_reward = 0\n",
        "    rewards_in_row = []\n",
        "    dirlist = os.listdir(HOME)\n",
        "    for i in range (len(dirlist)):\n",
        "      filename = dirlist[i]\n",
        "      if save_name in filename:\n",
        "        if '_' in filename: \n",
        "          try: \n",
        "            epoch = int((filename.split('_')[-1]).split('.')[-2])\n",
        "          except ValueError:\n",
        "            epoch = 0\n",
        "          if epoch > start_game:\n",
        "            start_game = epoch\n",
        "    if pd_start_game > start_game:\n",
        "      df.drop(index = [i for i in range(start_game+1, pd_start_game+1)], axis=0, inplace=True)\n",
        "    if start_game > 0:\n",
        "      file_path = os.path.join(HOME, f'{save_name}_{start_game}.h5')\n",
        "      model.load_state_dict(torch.load(file_path))\n",
        "      target_model.load_state_dict(torch.load(file_path))\n",
        "\n",
        "      print(f\"Loaded {file_path} save, for model & model_target. Starting training from {start_game}\")\n",
        "      # last epoch +1\n",
        "      epsilon = float(df.loc[(df[\"game_episode\"]==start_game), \"epsilon\"].item())\n",
        "      mean_reward = float(df.loc[(df[\"game_episode\"]==start_game), \"mean_reward\"].item())\n",
        "    else:\n",
        "      epsilon = 1.0\n",
        "      print(f\"Starting from scratches. Starting game = {start_game}\")\n",
        "      rewards_in_row = []\n",
        "    return df, start_game, epsilon, mean_reward, rewards_in_row\n",
        "\n",
        "df, start_game, epsilon, mean_reward, rewards_in_row = load_checkpoint(model_weights_save_name)\n",
        "\n",
        "if start_game != 0:\n",
        "  turn_idx = int(df.turns.max())\n",
        "  figshow(df)\n",
        "  game_episode = start_game\n",
        "  total_rewards = []\n",
        "  total_rewards = df.loc[(df[\"game_episode\"]<=start_game) &(df[\"game_episode\"]>=start_game-100), \"mean_reward\"].values.tolist()\n",
        "  # epsilon = 0.1\n",
        "  learning_rate = 0.00025\n",
        "  adam_hat = 1e-8\n",
        "  game_time = 0\n",
        "  if game_episode > 200:\n",
        "    lr_decay = 200/game_episode\n",
        "    learning_rate = learning_rate * lr_decay\n",
        "    if learning_rate < adam_hat:\n",
        "      learning_rate = adam_hat\n",
        "else:\n",
        "  turn_idx = 0\n",
        "  mean_reward = 0\n",
        "  game_episode = -1\n",
        "  epsilon = eps_start\n",
        "  total_rewards = []\n",
        "  learning_rate = 0.00025 \n",
        "\n",
        "print(df.tail(15).to_string())\n",
        "print(f'Start game episode:', start_game)\n",
        "print('Turn index:', turn_idx)\n",
        "print('Epsilon:', epsilon)\n",
        "print('Learning rate:', learning_rate)\n",
        "print('Mean reward on last game episode:', mean_reward)\n",
        "print(f'_mean_ of Total rewards for last 100 games: {np.mean(total_rewards[-100:]) if total_rewards else None}')\n",
        "\n",
        "exp_file_path = os.path.join(HOME, f'{exp_pkl_name}')\n",
        "if start_game != 0:\n",
        "  if os.path.exists(exp_file_path):\n",
        "    buffer.load(exp_file_path)\n",
        "  else:\n",
        "    msg = f\"Warning: pickle Exp file not found, starting without\" \n",
        "    print(msg)\n",
        "\n",
        "time_sum = int(df[\"time\"].sum())\n",
        "print(f'Model learning {start_game} games and {timedelta(seconds=time_sum)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nco3OLdg1ep5"
      },
      "source": [
        "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "msg_display = True\n",
        "best_mean_reward = None\n",
        "num_actions = 37\n",
        "loss_numpy = 0\n",
        "loss = 0\n",
        "start_datetime = datetime.now(timezone)\n",
        "print(\">>>Training starts at \", start_datetime)\n",
        "start_time = time.time()\n",
        "episodes_start_time = time.time()\n",
        "\n",
        "while True:\n",
        "        turn_idx += 1\n",
        "        if len(buffer) < replay_start_size:\n",
        "            epsilon = 1.0\n",
        "        reward = game_table.play_step(model, epsilon=epsilon, step_device=device)\n",
        "        if reward is not None:\n",
        "            epsilon = max(epsilon*eps_decay, eps_min)\n",
        "            reward = np.float16(reward)\n",
        "            game_episode += 1\n",
        "            total_rewards.append(reward)\n",
        "            if len(total_rewards) > 100:\n",
        "              mean_reward = np.mean(total_rewards[-100:])\n",
        "              del total_rewards[:1]\n",
        "            msg = f\"\\rgame_episode: {game_episode:07d}, turn: {turn_idx:09d}, epsilon: {epsilon:.3f}, \" \\\n",
        "                  f\"mean reward: {mean_reward:.3f}, loss: {loss_numpy:.4f}, \" \\\n",
        "                  f\"last reward {reward}\"\n",
        "\n",
        "            if (best_mean_reward is None or best_mean_reward < mean_reward) \\\n",
        "              and game_episode > 1000:\n",
        "                best_mean_reward = mean_reward\n",
        "                if best_mean_reward is not None:\n",
        "                    torch.save(model.state_dict(), os.path.join(HOME, f'{model_weights_save_name}_{game_episode:002d}.h5'))\n",
        "                    msg = f\"\\rgame_episode: {game_episode:07d}, turn: {turn_idx:09d}, epsilon: {epsilon:.3f}, \" \\\n",
        "                    f\"mean reward: {mean_reward:.3f}, loss: {loss_numpy:.4f}, \" \\\n",
        "                    f\"last reward {reward}\"\n",
        "                    msg = f'{msg}. Best mean reward updated {best_mean_reward:.3f}'\n",
        "                    msg_display = True\n",
        "\n",
        "            if mean_reward > MEAN_REWARD_BOUND \\\n",
        "              and len(buffer) > replay_start_size \\\n",
        "              and epsilon == 0.02:\n",
        "                end_time = time.time()\n",
        "                game_time = end_time - start_time \n",
        "                start_time  = time.time()\n",
        "                print(f'{msg}, time: {game_time:.4f} sec')    \n",
        "                print(\"Solved in %d turns!\" % turn_idx)\n",
        "                loss_numpy = loss_t.item()\n",
        "                # if tf.is_tensor(loss):\n",
        "                    # loss_numpy = tf.keras.backend.get_value(loss)\n",
        "                df = add_data_to_pd(df, game_episode, turn_idx, game_time, loss_numpy, epsilon, mean_reward)\n",
        "                df.to_csv(os.path.join(HOME, history_csv_name), encoding='utf-8')\n",
        "                break\n",
        "            end_time = time.time()\n",
        "            game_time = end_time - start_time \n",
        "            start_time  = time.time()\n",
        "\n",
        "            # if tf.is_tensor(loss):\n",
        "                # loss_numpy = tf.keras.backend.get_value(loss)    \n",
        "            if game_episode % 100 == 0 or msg_display:\n",
        "              episodes_end_time = time.time()\n",
        "              episodes_time = episodes_end_time - episodes_start_time \n",
        "              episodes_start_time  = time.time() \n",
        "              print(f'{msg}, episodes time: {episodes_time:.4f} sec')\n",
        "              msg_display = False\n",
        "            df = add_data_to_pd(df, game_episode, turn_idx, game_time, loss_numpy, epsilon, mean_reward)\n",
        "            if (game_episode % 500 == 0) and (game_episode != 0):\n",
        "                df.to_csv(os.path.join(HOME, history_csv_name), encoding='utf-8')\n",
        "            if (game_episode % 2500 == 0) and (game_episode != 0) and (turn_idx >= replay_start_size):\n",
        "                print('Saving weights...')\n",
        "                torch.save(model.state_dict(), os.path.join(HOME, f'{model_weights_save_name}_{game_episode:002d}.h5'))\n",
        "                buffer.save(exp_file_path, buffer_length=50000)\n",
        "            if game_episode % 1000 == 0:\n",
        "                figshow(df)\n",
        "            if (game_episode % 10000 == 0) and (game_episode != 0):  \n",
        "                clear_output()\n",
        "                print(df.tail(50).to_string())\n",
        "                figshow(df)\n",
        "        if len(buffer) < replay_start_size:\n",
        "            continue\n",
        "\n",
        "        batch = buffer.sample(batch_size)\n",
        "        states, actions, valid_actions_lst, rewards, dones, next_states = batch\n",
        "\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.LongTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        dones = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "        # resize tensors\n",
        "        actions = actions.view(actions.size(0), 1)\n",
        "        dones = dones.view(dones.size(0), 1)\n",
        "\n",
        "        # compute loss\n",
        "        curr_Q = model.forward(states).gather(1, actions)\n",
        "        next_Q = target_model.forward(next_states)\n",
        "        max_next_Q = torch.max(next_Q, 1)[0]\n",
        "        max_next_Q = max_next_Q.view(max_next_Q.size(0), 1)\n",
        "        # max_next_Q = max_next_Q.to(device)\n",
        "        expected_Q = rewards + (1 - dones) * gamma * max_next_Q\n",
        "        # expected_Q = expected_Q.to(device)\n",
        "\n",
        "        loss = F.mse_loss(curr_Q, expected_Q.detach())\n",
        "        # loss = nn.MSELoss()(curr_Q, expected_Q.detach)\n",
        "\n",
        "        # states_v = torch.tensor(states).to(device)\n",
        "        # next_states_v = torch.tensor(next_states).to(device)\n",
        "        # actions_v = torch.tensor(actions).to(device)\n",
        "        # rewards_v = torch.tensor(rewards).to(device)\n",
        "        # done_mask = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "        # state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # next_state_values = target_net(next_states_v).max(1)[0]\n",
        "\n",
        "        # next_state_values[done_mask] = -1.0\n",
        "\n",
        "        # next_state_values = next_state_values.detach()\n",
        "\n",
        "        # expected_state_action_values = next_state_values * gamma + rewards_v\n",
        "\n",
        "        # loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_numpy = loss.item()\n",
        "        \n",
        "        # target network update\n",
        "        if turn_idx % 2 == 0:\n",
        "          for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
        "              target_param.data.copy_(tau * param + (1 - tau) * target_param)\n",
        "\n",
        "        # if turn_idx % sync_target_turns == 0:\n",
        "            # update the the target network with new weights\n",
        "            # print(\"Synchronizing models...\")\n",
        "            # target_net.load_state_dict(net.state_dict())\n",
        "            # model_target.set_weights(model.get_weights())\n",
        "        # for target_param, param in zip(target_net.parameters(), net.parameters()):\n",
        "        #     target_param.data.copy_(tau * param + (1 - tau) * target_param)\n",
        "\n",
        "print('Saving weights...')\n",
        "torch.save(model1.state_dict(), os.path.join(HOME, f'{model_weights_save_name}_{game_episode:002d}.h5'))\n",
        "buffer.save(exp_file_path)\n",
        "figshow(df)\n",
        "\n",
        "end_datetime = datetime.now(timezone)\n",
        "print(\">>>Training ends at \", end_datetime)\n",
        "train_time = end_datetime-start_datetime\n",
        "print(f'Model trained:', train_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHsCAAc8T74g"
      },
      "source": [
        ""
      ]
    }
  ]
}